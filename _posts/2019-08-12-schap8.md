---
title: 第8章 提升方法
date: 2019-08-12 19:46:46 +0800
description:
image:
  path: /assets/images/posts/2019-08-12-schap8/cover.jpg
  thumbnail: /assets/images/posts/2019-08-12-schap8/thumb.jpg
categories:
  - it
tags:
  - 统计学习方法
---

## 1.1 本意概要

1. **提升方法**是将弱学习算法提升为强学习算法的统计学习方法。提升方法通过反复修改训练数据权值分布，构建一系列基本分类器（弱分类器），然后将这些弱分类器线性组合成一个强分类器。

2. AdaBoost 模型是弱分类器的线性组合：

   $$ f(x) = \sum\limits_{m=1}^M \alpha_m G_m(x)$$

3. AdaBoost 每次迭代提高前一轮分类器错误分类数据的权值，降低正确分类的数据权值。最后，将基本分类器线性组合，对于分类误差率小的基本分类器给予大的权值，给分类误差率大的基本分类器以小的权值

4. AdaBoost 每次迭代可以减少它在训练数据集上的分类误差率

5. AdaBoost 是前身分步算法的一个实现。模型是加法模型，损失函数是指数损失，算法是前身分步算法。在每步中极小化损失函数 $(\beta_m, \gamma_m) = arg \min\limits_{\beta, \gamma} \sum\limits_{i=1}^N L(y_i, f_{m-1}(x_i) + \beta b(x_i; \gamma))$ 得到参数 $\beta_m, \gamma_m$

6. 提升树是统计学习中最有效的方法之一。以分类树和回归树为基本分类器

## 1.2 目录

1. 提升方法  
   1.1 提升方法的基本思路  
   1.2 AdaBoost 算法  
   1.3 AdaBoost 的例子
2. AdaBoost 算法的训练误差分析
3. AdaBoost 算法的解释  
   3.1 前向分步算法  
   3.2 前向分步算法与 AdaBoost
4. 提升树  
   4.1 提升树模型  
   4.2 提升树算法  
   4.3 梯度提升

---

# 2 读书笔记

提升方法通过学习多个分类器，并将这些分类器进行线性组合，提高分类的性能

## 2.1 提升方法 AdaBoost

### 2.1.1 基本思路

- 对于复杂任务，三个臭皮匠顶个诸葛亮。

- **强可学习**是指对于一个类别（或概念）如果存在一个多项式的学习算法能够拟合它，并且正确率高

- **弱可学习**是指对于一个类别，算法学习的正确率仅比随机猜测略好

- **提升方法**是改变训练数据的概率分布（训练数据的权值分布），调用弱学习算法学习一系列弱分类器

### 2.1.2 AdaBoost 算法

- 输入：

  - 数据集 $T = \{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\} $
  - 弱学习算法

- 输出

  - 最终分类器 $G(x)$

- 流程

  1. 初始化训练数据集的权值分布

     $ D_1 = (w_{11}, ..., w_{1i}, ..., w_{1N})$，其中 $w_{1i} = \dfrac{1}{N}, i = 1, 2, ..., N$

  2. 对 $m = 1, 2, ...., M$

     - 用权值分布 $D_m$ 的训练数据集学习，得到基本分类器 $G_m(x): X \to \{-1, +1\}$

     - 计算 $G_m(x)$ 在训练数据集上的分类误差率

       $$ e_m = \sum\limits_{i=1}^{N} w_{mi} I(G_m(x_i) \neq y_i)$$

     - 计算 $G_m(x)$ 的系数 $\alpha_m = \dfrac{1}{2} \ln \dfrac{1 - e_m}{e_m}$

     - 更新训练数据集的权值分布

       $$ D_{m+1} = (w_{m+1, 1}, ..., w_{m+1, i, }..., w_{m+1, N})$$

       $$ w_{m+1, i} = \dfrac{w_{mi}}{Z_m} e^{-\alpha_m y_i G_m(x_i)}$$

       $$Z_m = \sum\limits_{i=1}^{N} w_{mi} e^{-\alpha_m y_i G_m(x_i)}$$

     - 构建基本分类器的线性组合 $f(x) = \sum\limits_{m=1}^N \alpha_m G_m(x)$，得到最终分类器：

       $$ G(x) = sign(f(x)) = sign \left( \sum\limits_{m=1}^N \alpha_m G_m(x)  \right)$$

## 2.2 AdaBoost 算法训练误差分析

### 2.2.1 AdaBoost 的训练误差边界

**定理 1** AdaBoost 算法最终分类器的训练误差界为：

$$
\dfrac{1}{N} \sum\limits_{i=1}^N I\left( G(x_i) \neq yi \right) \le \dfrac{1}{N} \sum\limits_i \exp(-y_i f(x_i)) = \prod_m Z_m
$$

**定理 2** AdaBoost 二分类问题的训练误差边界

$$
\prod_{m=1}^M Z_m = \prod_{m=1}^M 2 \sqrt{e_m(1-e_m)} = \prod_{m=1}^M \sqrt{1-4\gamma_m^2} \le \exp \left( -2 \sum\limits_{m=1}^M \gamma^2 \right)
$$

这里，$\gamma_m = \dfrac{1}{2} - e_m$

## 2.3 AdaBoost 算法的解释

从前向分步算法的角度理解 AdaBoost 算法

### 2.3.1 前向分步算法

**思想：**

从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标，就可以简化优化的复杂度

#### 算法

- 输入

  - 训练数据集 $ T = \{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N) \}$
  - 损失函数 $ L(y, f(x))$
  - 基函数集 $ \{ b(x; \gamma) \}$

- 输出

  - 加法模型 $f(x) = \sum\limits_{m=1}^M \beta_m b(x; \gamma)$

- 流程

  1. 初始化 $f_0(x) = 0$

  2. 对 $ m = 1, 2, ..., M$

     - 极小化损失函数得到参数 $\beta_m, \gamma_m$

       $$(\beta_m, \gamma_m) = arg\min\limits_{i=1}^N L\left( y_i, f_{m-1}(x_i) + \beta b(x; \gamma) \right)$$

     - 更新

       $$ f_m(x) = f_{m-1}(x) + \beta_m b(x; \gamma_m)$$

  3. 得到加法模型

     $$f(x) = \sum\limits_{m=1}^M \beta_m b(x; \gamma)$$

### 2.3.2 前向分步算法与 AdaBoost

- 加法模型： $ f(x) = \sum\limits_{m=1}^M \alpha_m G_m(x)$
- 损失函数：$L(y, f(x)) = \exp (-y f(x))$

- 基本分类器：$G_m(x)$

- 流程

  1. 经过 $m-1$ 轮迭代前向分步算法得到 $f_{m-1}(x)$

     $$ f_{m-1}(x) = f_{m-2}(x) + \alpha_{m-1} G_{m-1}(x) = \alpha_1 G_1(x) + ··· + \alpha_{m-1} G_{m-1}(x)$$

     则第 $m$ 轮迭代得到

     $$ f_m(x) = f_{m-1}(x) + \alpha_m G_m(x)$$

  2. 目标，最小化损失函数

     $$ \begin{split}\alpha_m, G_m(x) & = \arg \min\limits_{a, G} \sum\limits_{i=1}^N \exp [ -y_i \big(f_{m-1}(x_i) + \alpha G(x_i)\big)] \\ & = \arg\min\limits_{\alpha, G}\sum\limits_{i=1}^N \bar{w}_{mi} \exp [-y_i \alpha G(x_i)] \end{split}$$
