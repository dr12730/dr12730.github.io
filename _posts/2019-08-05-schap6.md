---
title: 逻辑回归与最大熵模型
date: 2019-08-05 21:30:30 +0800
description:
image:
  path: /assets/images/posts/2019-08-05-schap6/cover.jpg
  thumbnail: /assets/images/posts/2019-08-05-schap6/thumb.jpg
categories:
  - ai
tags:
  - 统计学习方法
---

<!-- vim-markdown-toc GFM -->

* [1 前言](#1-前言)
    * [1.1 本章概要](#11-本章概要)
    * [1.2 目录](#12-目录)
* [2 读书笔记](#2-读书笔记)
    * [2.1 逻辑回归模型](#21-逻辑回归模型)
        * [2.1.1 逻辑分布](#211-逻辑分布)
        * [2.1.2 二项逻辑回归模型](#212-二项逻辑回归模型)
            * [几率](#几率)
        * [2.1.3 模型的参数估计](#213-模型的参数估计)
        * [2.1.4 多项逻辑回归模型](#214-多项逻辑回归模型)
    * [2.2 最大熵模型](#22-最大熵模型)
        * [2.2.1 最大熵原理](#221-最大熵原理)
        * [2.2.2 最大熵模型的定义](#222-最大熵模型的定义)
        * [2.2.3 最大熵模型的学习](#223-最大熵模型的学习)
* [3 导读笔记](#3-导读笔记)
    * [3.1 逻辑回归模型](#31-逻辑回归模型)
        * [求解 w](#求解-w)
    * [3.2 最大熵模型](#32-最大熵模型)
    * [3.2 拉格朗日对偶性](#32-拉格朗日对偶性)
* [4 总结](#4-总结)
    * [4.1 什么是梯度下降？](#41-什么是梯度下降)
    * [4.2 什么是 Logistic 回归？](#42-什么是-logistic-回归)

<!-- vim-markdown-toc -->

# 1 前言

## 1.1 本章概要

1. 逻辑回归模型是用下面分布表示的模型，用于分类

   $$P(Y = k \mid x) = \dfrac{e^{w_k \cdot x}} {1 + \sum\limits_{k=1}^{K-1} e^{w_k \cdot x}} $$

   $$P(Y=K \mid x) = \dfrac{1}{1+\sum\limits_{k=1}^{K-1} e^{w_k \cdot x}}$$

   其中
   $k = 1, 2, ..., K-1$

   > 逻辑回归模型是用输入的线性函数表示输出的对数几率模型

2. 最大熵模型是用下面分布表示的分类模型

   $$ P_w(y \mid x) = \dfrac{1}{Z_w(x)} e ^{\sum\limits_{i=1}^{n} w_if_i(x, y)} $$

   $$ Z_w(x) = \sum\limits_y e^{\sum\limits_{i=1}^{n}w_if_i(x, y)} $$

   - $Z_w(x)$ 是归一化因子
   - $f_i$ 是特征函数
   - $w_i$ 特征权值

3. 最大熵原理认为在所有可能的概率模型（分布）的集合中，熵最大的模型是最好的模型

4. 最大熵原理应用到分类模型学习中，有以下约束最优化问题：

   $$ min\{-H(P)\} = \sum\limits_{(x, y)} \widetilde{P}(x)P(y \mid x)logP(y \mid x)$$

   约束条件：

   $$ P(f_i) - \widetilde{P}(f_i) = 0 $$

   $$ \sum\limits_{y} P(y \mid x) = 1$$

   求解此优化问题的对偶问题得到最大熵模型

5. 逻辑回归模型和最大熵模型都是对数线性模型

6. 逻辑回归模型和最大熵模型一般用极大似然估计，或正则化的极大似然估计

7. 求解最优化问题的算法有迭代尺度法、梯度下降法、拟牛顿法

## 1.2 目录

1. 逻辑回归模型

   1.1 逻辑分布

   1.2 二项逻辑回归模型

   1.3 模型参数估计

   1.4 多项逻辑回归

2. 最大熵模型

   2.1 最大熵原理

   2.2 最大熵模型的定义

   2.3 最大熵模型的学习

   2.4 极大似然估计

3. 模型学习的最优化算法

   3.1 改进的迭代尺度法

   3.2 拟牛顿法

---

# 2 读书笔记

## 2.1 逻辑回归模型

### 2.1.1 逻辑分布

设 $X$ 是连续随机变量， $X$ 服从逻辑分布是指 $X$ 具有以下分布函数和密度函数：

$$ F(x) = P(X \le x) = \dfrac{1}{1 + e^{-(x-\mu/\gamma)} }$$

$$ f(x) = F'(x) = \dfrac{e^{-(x-\mu)/\gamma}}{\gamma\left[1+e^{-(x-\mu)/\gamma}\right]^2}$$

式中， $\mu$ 为位置参数，$\gamma > 0$ 为形状参数

逻辑分布的密度函数和分布函数如下：

![logit](/assets/images/posts/2019-08-05-schap6/log.png)

### 2.1.2 二项逻辑回归模型

二项逻辑回归模型是如下的条件概率分布：

$$P(Y =1 \mid x) = \dfrac{e^{(w \cdot x _+ b)}}{1 + e^{(w \cdot x + b)}} $$

$$ P(Y = 0 \mid x) = \dfrac{1} {1 + e^{w \cdot x + b}}$$

对于新的输入 $x$，求出 $P(Y = 1 \mid x)$ 和 $P(Y = 0 \mid x)$，求实例 $x$ 分配到概率值大的一类

#### 几率

事件发生的概率与不发生概率的比值 $\dfrac{p}{1-p}$

对数几率或 logit 函数是：

$$logit(p) = log \dfrac{p}{1-p}$$

对数几率的线性模型表示为：

$$ log \dfrac{P(Y=1 \mid x)}{1 - P(Y = 1 \mid x)} = w \cdot x$$

由此可求出逻辑回归模型：

$$ P(Y = 1 \mid x) = \dfrac{e^{w \cdot x}}{1 + e^{w \cdot x}} $$

$$  P(Y = 0 \mid x) = \dfrac{1}{1 + e^{w \cdot x}}  $$

### 2.1.3 模型的参数估计

极大似然估计法估计参数，对于 0-1 分布的样本，有

$$P(Y = 1 \mid x) = \pi(x)$$

$$P(Y = 0 \mid x) = 1 - \pi(x)$$

对应的似然函数为：

$$ L(w) = \prod\limits_{i=1}^N = [\pi(x_i)]^{y_i}[1 - \pi(x_i)]^{1-y_i}$$

再求对数似然函数的极大值（梯度下降或拟牛顿法），可得 w 的估计值

当求出 w 的极大似然估计值 $\hat{w}$ 后，逻辑回归模型为：

$$P(Y = 1 \mid x) = \dfrac {e^{\hat{w} \cdot x}}{1+ e^{\hat{w} \cdot x}}$$  
$$P(Y = 0 \mid x) = \dfrac {1}{1+ e^{\hat{w} \cdot x}}$$

### 2.1.4 多项逻辑回归模型

模型为：

$$P(Y = k \mid x) = \dfrac {e^{w_k \cdot x}}{1+ \sum\limits_{k=1}^{K-1} e^{w_k \cdot x}}$$  
$$P(Y = K \mid x) = \dfrac {1}{1+ \sum\limits_{k=1}^{K-1} e^{w_k \cdot x}}$$

## 2.2 最大熵模型

则最大熵原理推导

### 2.2.1 最大熵原理

最大熵原理认为在所有可能的概率模型（分布）的集合中，熵最大的模型是最好的模型。约束条件确定概率模型的集合

### 2.2.2 最大熵模型的定义

假设满足所有的**约束条件**的模型集合为：

$$ M = \{ P \in \Gamma \mid E_p(f_i) = E_{\widetilde{P}}(f_i) \}$$

定义在条件概率分布 $P(Y \mid X)$ 上的条件熵为：

$$H(P) = - \sum\limits_{x, y} \widetilde{P}(x)P(y \mid x) logP(y \mid x)$$

则模型集合 $M$ 中条件熵 $H(P)$ 最大的模型成为 **最大熵模型**

### 2.2.3 最大熵模型的学习

最大熵模型的学习过程就是模型的求最优解过程。所以学习过程可以转化成约束最优化过程

对于训练集 $T = \{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N) \} $ 以及特征函数 $ f_i(x, y)$，最大熵模型的学习等价于约束最优化问题：

$$ \max\limits_{P \in M} H(P) = - \sum\limits_{x,y} \widetilde{P}(x)P(y \mid x)logP(y \mid x)$$

约束条件：

$$ E_P(f_i) = E_{\widetilde{P}}(f_i), i = 1, 2, ..., n$$

$$ \sum\limits_y P(y \mid x) = 1$$

按惯例转化为求最小值问题：

$$ \min\limits_{P \in M} - H(P) = \sum\limits_{x,y} \widetilde{P}(x)P(y \mid x)logP(y \mid x)$$

约束条件：

$$ E_P(f_i) - E_{\widetilde{P}}(f_i) = 0, i = 1, 2, ..., n$$

$$ \sum\limits_y P(y \mid x) = 1$$

上面式子的解就是最大熵模型学习的解，其解为：

$$ w* = arg\max\limits_w \Psi(w)$$

这里：

$$ \Psi(w) = \min\limits_{P \in M} L(P, w) = L(P_w, w)$$

$L$ 是拉格朗日函数：

$$ L(P, w) = -H(P) + w_0 \left( 1 - \sum\limits_yP(y \mid x) \right) + \sum\limits_{i=1}^n w_i (E_{\widetilde{P}}(f_i) - E_p(f_i))$$

---

# 3 导读笔记

## 3.1 逻辑回归模型

对数线性模型的形式为： $logP(y \mid x) = w \cdot x$

对于给定的 $x$，我们希望把它的类别概率表示为 $P(y = 1 \mid x) = \pi(x)$，其中 $\pi(x) \in [0, 1]$

为了用线性函数 $w \cdot x$ 来表示 $\pi(x)$ 的特征，需要一个变换，把 $(-\infty, +\infty) \to [0, 1]$，用到了 logit 变换：

$$ logit(\pi(x)) = log \dfrac{\pi(x)} {1 - \pi(x)} \in (-\infty, +\infty)$$
$$ log \dfrac{\pi(x)} {1 - \pi(x)} = w \cdot x$$

反解出逻辑回归模型：

$$ \pi(x) = \dfrac{e^{wx}}{1 + e^{wx}} = P(Y = 1 \mid x) $$
$$ \pi(x) = \dfrac{1}{1 + e^{wx}} = P(Y = 0 \mid x) $$

只需要求出 $w$ 就有了模型，然后对于新的 $x$，求出对应的 $P(Y=1 \mid x)$，当 $P(Y=1 \mid x) > 0.5$ 时，判定分类为 1

### 求解 w

用极大似然估计法求解 $w$：给定 w，求样本的联合概率密度，让它最大，即可求出 w

对于 $y_i \in \{0, 1\}$，给定 $\pi(x)$ 后，$y$ 的概率分布为：

$$ P_w(y \mid x) = \pi(x)^y [1 - \pi(x)]^{(1-y)} $$

对于 N 个样本的极大似然估计函数 $L(w)$：

$$L(w) = \prod\limits_{i=1}^N \pi(x)^{y_i} [1 - \pi(x)]^{(1-y_i)}$$

只要求 $\max logL(w)$ 的 $w$ 即可。一般用梯度下降法，给定一个初值 $w_0$ 即可找到 $w*$

## 3.2 最大熵模型

**原理：**在满足约束条件的模型集合中，选择熵最大的模型（混乱程度最大的模型）。因为在条件不足时，只能假设各种情况出现的概率相同

例如：对于 $X \in \{A, B, C, D, E\}$，估计各值出现的概率。

约束条件：

$$ \sum\limits_1^5 P(x_i) = 1 $$

$$P(x_i) \ge 0 $$

在满足约束的模型中找最乱的，所以各概率相同，所以有 $P(x_i) = 1/5$

对于新的约束条件：

$$ P(A) + P(B) = \dfrac{3}{10} $$

如何找集合中熵最大的模型

因为熵 $H(p) = - \sum p_i log p_i,\max H = \min [-H]$，所以：

要求的模型等价于求最优问题：

$$\min[-H(p)] = \sum\limits_{i=1}^5 P(y_i) \log P(y_i)$$

约束条件：

$$  \sum\limits_1^5 P(x_i) = 1  $$

$$P(x_i) \ge 0 $$

$$  P(A) + P(B) = \dfrac{3}{10}  $$

我们 $x$ 是已知的，如何把已知的 $x$ 信息加入来求 $p(y)$？

用条件熵：

$$ H(p) = -\sum\limits_{x, y} \widetilde{p}(x)p(y \mid x) \log P(y \mid x)$$

因为熵 $H(p) = -\sum p_i \log p_i$

$$H(y \mid x) = - \sum p(y \mid x) \log p(y \mid x)$$

所以：

$$ H(p) = E_x H(y \mid x) = \sum\limits_{x, y} p(x) p(y \mid x) \log p(y \mid x)$$

我们希望总体 $x$ 的分布用样本上的 $x$ 分布（也称经验分布）来代替，即

$$ \sum\limits_{x, y} p(x) p(y \mid x) \log p(y \mid x) = \sum\limits_{x, y} \widetilde{p}(x)p(y \mid x) \log P(y \mid x) $$

就要引入约束条件：

$$ E_{p(x, y)} f_i(x, y) = E_{\widetilde{p}(x, y)}f_i(x, y) $$

上式的 $p(x, y)$ 为联合概率分布，上式的含义是让每个特征在总体中出现的概率 = 样本中出现的概率

其中 $f_i(x, y)$ 是特征函数，代表观察到的信息

这样最大熵模型对应的最优化问题如下：

$$\max\limits_{p \in M} H(p) = - \sum\limits_{x, y} \widetilde{p}(x) p(y \mid x) \log p(y \mid x)$$

约束条件 n+1 个：

$$ E_p(f_i) = E_{\widetilde{p}}(f_i)$$

$$ \sum\limits_y p(y \mid x) = 1$$

其中：$i = 1, 2, ..., n$

最优问题的求解结果为：

$$ P_w(y \mid x) = \dfrac{1}{Z_w(x)} e^{\sum\limits_{i=1}^n w_if_i} $$

其中：

$$w = arg\max L_{\widetilde{p}} (P_w) = \log \pi(y \mid x)$$

求解的流程：

1. 给定一个 $x$，求 $y$ 取不同值时对应的概率分布
2. $(x, y)$ 满足 n 个 $f_i$ 的要求，$w_i$ 是特征 $f_i$ 的重要程度，当满足的特征越多且特征越重要，$P_w$ 值越大
3. 求 $w$，代入 $P_w(y \mid x)$ 就得到模型

## 3.2 拉格朗日对偶性

对于任意的优化问题，有一个优化的目标函数，记为 $f$，需要优化的变量记为 $x$，约束最优化问题一般形式为：

$$\min\limits_{x \in R^n} f(x)$$

约束条件：

$$ c_i(x) \le 0,  i = 1, 2, ..., k$$

$$h_j(x) = 0, j =1,2, ..., l$$

$c_i$ 称为不等式约束，$h$ 称为等式约束

以上的原始问题记为 $P$，它对应一个拉格朗日函数：

$$L(x, \alpha, \beta) = f(x) + \sum\limits_{i =1}^k \alpha_i c_i(x) + \sum\limits_{j=1}^l \beta_j h_j(x)$$

其中：$\alpha$ 称为不等式约束因子，$\beta$ 称为等式约束因子，$\alpha_i$ 对应第 $i$ 个不等式约束，$\alpha_i \ge 0$，$\beta_j$ 不作要求。

满足约束条件的 $x$ 的范围称为可行域，即把 $k+l$ 个集合求交集。

优化问题就是在可行域中，找到 $x$ 使 $f$ 最小，对应的结果就是 $x*, P*$，用数学表达式写作：

$$
P = \min\limits_x \max\limits_{\alpha, \beta} L(x, \alpha, \beta) = \left\{
\begin{aligned}
f(x) && c_i(x) \le 0, h_j(x) = 0 \\
\infty && 其他 \\
\end{aligned}
\right.
$$

也就是满足尽可能多的约束下，让 $f$ 取到最小值

> 1. 当 $c_i, h_j$ 无约束时，总可以让后面的部分为 $\infty$
> 2. 当 $c_i(x) \le 0, h_j(x) = 0$ 时，$L$ 要最大，第 3 项为 0 第 2 项小于 0，所以最大为 $f(x)$

因此，原始问题等价于极小极大化 $L$ 函数，所以原始问题的对偶性问题可以写成：

$$ \max\limits_{\alpha, \beta} \min\limits_x L$$

首先求无约束问题，再求有约束最优问题

因此，拉格朗日对偶问题为：

$$
\max\limits_{\alpha, \beta} \min\limits_x L(x, \alpha, \beta)
$$

约束条件 $ \alpha_i \ge 0$，最优解：$\alpha^*, \beta^*$，最优值 $d^*$

下面再看 $P^*$ 和 $d^*$ 的关系

$$
d^* =\max\limits_{\alpha, \beta} \min\limits_x L(x, \alpha, \beta) \le \max\limits_{\alpha, \beta} \min\limits_{x \in可行域} L(x, \alpha, \beta) \le \max\limits_{\alpha, \beta} \min\limits_{x \in可行域}f(x) = \min\limits_{x \in可行域}f(x) = P^*
$$

对偶问题的最优值给原始问题的最优值提供了下界，$P^* \ge d^*$

什么时候 $P^* = d^*$ ? 当原始问题满足：

# 4 总结

## 4.1 什么是梯度下降？

1. 梯度方向是函数增长最快的方向，负梯度是函数减小最快的方向
2. 要求函数的最小值，可以从任意点 $X_0 = (x_1, x_2, ..., x_n)$ 开始进行一个迭代过程
   $$x^{(i+1)}_1 = x^{(i)}_1 - lr * \frac{∂f}{∂x_1}$$
   $$x^{(i+1)}_2 = x^{(i)}_2 - lr * \frac{∂f}{∂x_2}$$
   $$... ...$$
   $$x^{(i+1)}_n = x^{(i)}_n - lr * \frac{∂f}{∂x_n}$$
   一直到收敛条件满足

## 4.2 什么是 Logistic 回归？

可以想成对数几率【拟合】，就是用线性函数 wx 去拟合对数几率

- 为什么是对数几率？

  $$ ∵  wx = \log \frac{p}{1-p} > log 1 = 0$$

  所以当 $wx > 0$ 是可以知道 $\dfrac{p}{1-p} > 1$，就说明样本 $x$ 是正样本

- 为什么 wx > 0 时，x 是正样本？

  因为 $wx = \log \dfrac{p}{1-p}$，这里 $p = p(Y=1 \mid x)$

  那么当 $p>0.5$ 时，$\dfrac{p}{1-p}>1$ , $\log \dfrac{p}{1-p} > 0$

- 什么是 Logistic 回归模型？

  Logistic 回归是统计学习中的分类方法

  - 统计学习

    计算机基于数据构建概率统计模型，并用模型对数据进行预测与分析的学科

  - 统计学习的方法

    在假设数据集是独立同分布产生的前提下，应用某个评价标准，从假设空间中根据最优算法选择一个最优的模型。统计学习方法的三要素：模型的假设空间、模型选择的标准、模型学习的算法

  Logistic 回归模型是利用 Logistic 回归方法得到的一个线性分类模型，这个线性模型的假设空间是线性空间 wx，这个线性模型的评价准则是极大似然估计，这个线性模型的最优算法是用梯度下降法

  > 假设空间是输入空间到输出空间的映射的集合

- Logistic 回归的由来：

  希望通过 wx 直接预测 x 属于正样本的概率。但 wx 值域无界，所以用 sigmoid 函数把 wx 映射到[0, 1]，所以有 $p(Y=1 \mid x) = sigmoid(x)$

- 极大似然估计

  对参数的求解采用了极大似然估计，由数据集的分布
  $$ p(y=1 \mid x) = sigmoid(x) = \dfrac{1}{1 + e^{-wx}}$$
  $$ p(y=0 \mid x) = 1 - sigmoid(x) = \dfrac{e^{-wx}}{1 + e^{-wx}}$$
  可以将分布统一写成：
  $$p(y \mid x, w) = h^y(x)(1-h(x))^{1-y}$$
  那么对于训练样本 $D = {(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)}$，有极大似然函数：
  $$L(w) = \prod\limits_{i=1}^N \pi(x)^{y_i} [1 - \pi(x)]^{(1-y_i)}$$
  其中：
  $$P(Y = 1 \mid x) = \pi(x)$$

  所以它的对数似然估计就是：
  $$ \log L(w) = \sum\limits_{i=1}^N (y_i \log h(x_i) + (1-y_i)\log (1-h(x_i)))$$
  那么现在要求最大似然估计值，等价于求如下最小问题：
  $$ \min\limits_w (-f(w))$$
  这个就是学习方法的**评价标准**

- 梯度下降法

  这是最优算法，寻找最优模型。目标函数的梯度为：

  $$ \begin{aligned} - \nabla f(w) &= - \nabla \left(\sum\limits_{i=1}^N (y_i \log h(x_i) + (1-y_i)\log (1-h(x_i)))\right)  \\ &= \sum\limits_{i=1}^N \left(h(x_i) - y_i \right)x_i \end{aligned}$$

  得到梯度后，w 的更新公式：
  $$ w_{k+1} = w_k - \alpha \sum\limits_{i=1}^N \left(h_w(x_i) - y_i \right)x_i$$
