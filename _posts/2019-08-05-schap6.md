---
title:
date: 2019-08-05 21:30:30 +0800
description:
image:
  path: /assets/images/posts/2019-08-05-schap6/cover.jpg
  thumbnail: /assets/images/posts/2019-08-05-schap6/thumb.jpg
categories:
  - ai
tags:
  - 统计学习方法
---

# 第 6 章 逻辑回归与最大熵模型

## 1.1 本章概要

1. 逻辑回归模型是用下面分布表示的模型，用于分类

   $$P(Y = k \mid x) = \dfrac{e^{w_k \cdot x}} {1 + \sum\limits_{k=1}^{K-1} e^{w_k \cdot x}} $$

   $$P(Y=K \mid x) = \dfrac{1}{1+\sum\limits_{k=1}^{K-1} e^{w_k \cdot x}}$$

   其中
   $k = 1, 2, ..., K-1$

   > 逻辑回归模型是用输入的线性函数表示输出的对数几率模型

2. 最大熵模型是用下面分布表示的分类模型

   $$ P_w(y \mid x) = \dfrac{1}{Z_w(x)} e ^{\sum\limits_{i=1}^{n} w_if_i(x, y)} $$

   $$ Z_w(x) = \sum\limits_y e^{\sum\limits_{i=1}^{n}w_if_i(x, y)} $$

   - $Z_w(x)$ 是归一化因子
   - $f_i$ 是特征函数
   - $w_i$ 特征权值

3. 最大熵原理认为在所有可能的概率模型（分布）的集合中，熵最大的模型是最好的模型

4. 最大熵原理应用到分类模型学习中，有以下约束最优化问题：

   $$ min\{-H(P)\} = \sum\limits_{(x, y)} \widetilde{P}(x)P(y \mid x)logP(y \mid x)$$

   约束条件：

   $$ P(f_i) - \widetilde{P}(f_i) = 0 $$

   $$ \sum\limits_{y} P(y \mid x) = 1$$

   求解此优化问题的对偶问题得到最大熵模型

5. 逻辑回归模型和最大熵模型都是对数线性模型

6. 逻辑回归模型和最大熵模型一般用极大似然估计，或正则化的极大似然估计

7. 求解最优化问题的算法有迭代尺度法、梯度下降法、拟牛顿法

## 1.2 目录

1. 逻辑回归模型

   1.1 逻辑分布

   1.2 二项逻辑回归模型

   1.3 模型参数估计

   1.4 多项逻辑回归

2. 最大熵模型

   2.1 最大熵原理

   2.2 最大熵模型的定义

   2.3 最大熵模型的学习

   2.4 极大似然估计

3. 模型学习的最优化算法

   3.1 改进的迭代尺度法

   3.2 拟牛顿法

---

## 2.1 逻辑回归模型

### 2.1.1 逻辑分布

设 $X$ 是连续随机变量， $X$ 服从逻辑分布是指 $X$ 具有以下分布函数和密度函数：

$$ F(x) = P(X \le x) = \dfrac{1}{1 + e^{-(x-\mu/\gamma)} }$$
$$ f(x) = F'(x) = \dfrac{e^{-(x-\mu)/\gamma}}{\gamma\left[1+e^{-(x-\mu)/\gamma}\right]^2}$$

式中， $\mu$ 为位置参数，$\gamma > 0$ 为形状参数

逻辑分布的密度函数和分布函数如下：

![logit](/assets/images/posts/2019-08-05-schap6/log.png)

### 2.1.2 二项逻辑回归模型

二项逻辑回归模型是如下的条件概率分布：

$$P(Y =1 \mid x) = \dfrac{e^{(w \cdot x _+ b)}}{1 + e^{(w \cdot x + b)}} $$
$$ P(Y = 0 \mid x) = \dfrac{1} {1 + e^{w \cdot x + b}}$$

对于新的输入 $x$，求出 $P(Y = 1 \mid x)$ 和 $P(Y = 0 \mid x)$，求实例 $x$ 分配到概率值大的一类

#### 几率

一个事件的 **几率** 是指该事件发生的概率与不发生概率的比值 $\dfrac{p}{1-p}$，它的对数几率或 logit 函数是：
$$logit(p) = log \dfrac{p}{1-p}$$

对数几率的线性模型表示为：

$$ log \dfrac{P(Y=1 \mid x)}{1 - P(Y = 1 \mid x)} = w \cdot x$$

由此可求出逻辑回归模型：

$$ P(Y = 1 \mid x) = \dfrac{e^(w \cdot x)}{1 + e^{(w \cdot x)}} $$

### 2.1.3 模型的参数估计

极大似然估计法估计参数，对于 0-1 分布的样本，有 $P(Y = 1 \mid x) = \pi(x), P(Y = 0 \mid x) = 1 - \pi(x)$，对应的似然函数为：

$$ L(w) = \prod\limits_{i=1}^N = [\pi(x_i)]^{y_i}[1 - \pi(x_i)]^{1-y_i}$$

再求对数似然函数，再求极大值（梯度下降或拟牛顿法），可得 w 的估计值

当求出 w 的极大似然估计值 $\hat{w}$ 后，逻辑回归模型为：

$$P(Y = 1 \mid x) = \dfrac {e^{(\hat{w} \cdot x)}}{1+ e^{(\hat{w} \cdot x)}}$$
$$P(Y = 0 \mid x) = \dfrac {1}{1+ e^{(\hat{w} \cdot x)}}$$

### 2.1.4 多项逻辑回归模型

模型为：

$$P(Y = k \mid x) = \dfrac {e^{(w_k \cdot x)}}{1+ \sum\limits_{k=1}^{K-1} e^{(w_k \cdot x)}}$$
$$P(Y = K \mid x) = \dfrac {1}{1+ \sum\limits_{k=1}^{K-1} e^{(w_k \cdot x)}}$$

## 2.2 最大熵模型

则最大熵原理推导

### 2.2.1 最大熵原理

最大熵原理认为在所有可能的概率模型（分布）的集合中，熵最大的模型是最好的模型。约束条件确定概率模型的集合

### 2.2.2 最大熵模型的定义

假设满足所有的**约束条件**的模型集合为：

$$ M = \{ P \in \Gamma \mid E_p(f_i) = E_{\widetilde{P}}(f_i) \}$$

定义在条件概率分布 $P(Y \mid X)$ 上的条件熵为：

$$H(P) = - \sum\limits_{x, y} \widetilde{P}(x)P(y \mid x) logP(y \mid x)$$

则模型集合 $M$ 中条件熵 $H(P)$ 最大的模型成为 **最大熵模型**

### 2.2.3 最大熵模型的学习

最大熵模型的学习过程就是模型的求最优解过程。所以学习过程可以转化成约束最优化过程

对于训练集 $T = \{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N) \} $ 以及特征函数 $ f_i(x, y)$，最大熵模型的学习等价于约束最优化问题：

$$ \max\limits_{P \in M} H(P) = - \sum\limits_{x,y} \widetilde{P}(x)P(y \mid x)logP(y \mid x)$$
约束条件：

$$ E_P(f_i) = E_{\widetilde{P}}(f_i), i = 1, 2, ..., n$$
$$ \sum\limits_y P(y \mid x) = 1$$

按惯例转化为求最小值问题：

$$ \min\limits_{P \in M} - H(P) = \sum\limits_{x,y} \widetilde{P}(x)P(y \mid x)logP(y \mid x)$$

约束条件：

$$ E_P(f_i) - E_{\widetilde{P}}(f_i) = 0, i = 1, 2, ..., n$$
$$ \sum\limits_y P(y \mid x) = 1$$

上面式子的解就是最大熵模型学习的解，其解为：

$$ w* = arg\max\limits_w \Psi(w)$$

这里：

$$ \Psi(w) = \min\limits_{P \in M} L(P, w) = L(P_w, w)$$

$L$ 是拉格朗日函数：

$$ L(P, w) = -H(P) + w_0 \left( 1 - \sum\limits_yP(y \mid x) \right) + \sum\limits_{i=1}^n w_i (E_{\widetilde{P}}(f_i) - E_p(f_i))$$
