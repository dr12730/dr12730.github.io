---
title: 逻辑回归与最大熵模型
date: 2019-08-05 21:30:30 +0800
description:
image:
  path: /assets/images/posts/2019-08-05-schap6/cover.jpg
  thumbnail: /assets/images/posts/2019-08-05-schap6/thumb.jpg
categories:
  - ai
tags:
  - 统计学习方法
---

## 1.1 本章概要

1. 逻辑回归模型是用下面分布表示的模型，用于分类

   $$P(Y = k \mid x) = \dfrac{e^{w_k \cdot x}} {1 + \sum\limits_{k=1}^{K-1} e^{w_k \cdot x}} $$

   $$P(Y=K \mid x) = \dfrac{1}{1+\sum\limits_{k=1}^{K-1} e^{w_k \cdot x}}$$

   其中
   $k = 1, 2, ..., K-1$

   > 逻辑回归模型是用输入的线性函数表示输出的对数几率模型

2. 最大熵模型是用下面分布表示的分类模型

   $$ P_w(y \mid x) = \dfrac{1}{Z_w(x)} e ^{\sum\limits_{i=1}^{n} w_if_i(x, y)} $$

   $$ Z_w(x) = \sum\limits_y e^{\sum\limits_{i=1}^{n}w_if_i(x, y)} $$

   - $Z_w(x)$ 是归一化因子
   - $f_i$ 是特征函数
   - $w_i$ 特征权值

3. 最大熵原理认为在所有可能的概率模型（分布）的集合中，熵最大的模型是最好的模型

4. 最大熵原理应用到分类模型学习中，有以下约束最优化问题：

   $$ min\{-H(P)\} = \sum\limits_{(x, y)} \widetilde{P}(x)P(y \mid x)logP(y \mid x)$$

   约束条件：

   $$ P(f_i) - \widetilde{P}(f_i) = 0 $$

   $$ \sum\limits_{y} P(y \mid x) = 1$$

   求解此优化问题的对偶问题得到最大熵模型

5. 逻辑回归模型和最大熵模型都是对数线性模型

6. 逻辑回归模型和最大熵模型一般用极大似然估计，或正则化的极大似然估计

7. 求解最优化问题的算法有迭代尺度法、梯度下降法、拟牛顿法

## 1.2 目录

1. 逻辑回归模型

   1.1 逻辑分布

   1.2 二项逻辑回归模型

   1.3 模型参数估计

   1.4 多项逻辑回归

2. 最大熵模型

   2.1 最大熵原理

   2.2 最大熵模型的定义

   2.3 最大熵模型的学习

   2.4 极大似然估计

3. 模型学习的最优化算法

   3.1 改进的迭代尺度法

   3.2 拟牛顿法

---

# 2 读书笔记

## 2.1 逻辑回归模型

### 2.1.1 逻辑分布

设 $X$ 是连续随机变量， $X$ 服从逻辑分布是指 $X$ 具有以下分布函数和密度函数：

$$ F(x) = P(X \le x) = \dfrac{1}{1 + e^{-(x-\mu/\gamma)} }$$  

$$ f(x) = F'(x) = \dfrac{e^{-(x-\mu)/\gamma}}{\gamma\left[1+e^{-(x-\mu)/\gamma}\right]^2}$$

式中， $\mu$ 为位置参数，$\gamma > 0$ 为形状参数

逻辑分布的密度函数和分布函数如下：

![logit](/assets/images/posts/2019-08-05-schap6/log.png)

### 2.1.2 二项逻辑回归模型

二项逻辑回归模型是如下的条件概率分布：

$$P(Y =1 \mid x) = \dfrac{e^{(w \cdot x _+ b)}}{1 + e^{(w \cdot x + b)}} $$  

$$ P(Y = 0 \mid x) = \dfrac{1} {1 + e^{w \cdot x + b}}$$

对于新的输入 $x$，求出 $P(Y = 1 \mid x)$ 和 $P(Y = 0 \mid x)$，求实例 $x$ 分配到概率值大的一类

#### 几率

事件发生的概率与不发生概率的比值 $\dfrac{p}{1-p}$  

对数几率或 logit 函数是：

$$logit(p) = log \dfrac{p}{1-p}$$  

对数几率的线性模型表示为：

$$ log \dfrac{P(Y=1 \mid x)}{1 - P(Y = 1 \mid x)} = w \cdot x$$

由此可求出逻辑回归模型：

$$ P(Y = 1 \mid x) = \dfrac{e^{w \cdot x}}{1 + e^{w \cdot x}} $$    

$$  P(Y = 0 \mid x) = \dfrac{1}{1 + e^{w \cdot x}}  $$

### 2.1.3 模型的参数估计

极大似然估计法估计参数，对于 0-1 分布的样本，有

 $$P(Y = 1 \mid x) = \pi(x)$$  

 $$P(Y = 0 \mid x) = 1 - \pi(x)$$

对应的似然函数为：

$$ L(w) = \prod\limits_{i=1}^N = [\pi(x_i)]^{y_i}[1 - \pi(x_i)]^{1-y_i}$$

再求对数似然函数的极大值（梯度下降或拟牛顿法），可得 w 的估计值

当求出 w 的极大似然估计值 $\hat{w}$ 后，逻辑回归模型为：

$$P(Y = 1 \mid x) = \dfrac {e^{\hat{w} \cdot x}}{1+ e^{\hat{w} \cdot x}}$$  
$$P(Y = 0 \mid x) = \dfrac {1}{1+ e^{\hat{w} \cdot x}}$$

### 2.1.4 多项逻辑回归模型

模型为：

$$P(Y = k \mid x) = \dfrac {e^{w_k \cdot x}}{1+ \sum\limits_{k=1}^{K-1} e^{w_k \cdot x}}$$  
$$P(Y = K \mid x) = \dfrac {1}{1+ \sum\limits_{k=1}^{K-1} e^{w_k \cdot x}}$$

## 2.2 最大熵模型

则最大熵原理推导

### 2.2.1 最大熵原理

最大熵原理认为在所有可能的概率模型（分布）的集合中，熵最大的模型是最好的模型。约束条件确定概率模型的集合

### 2.2.2 最大熵模型的定义

假设满足所有的**约束条件**的模型集合为：

$$ M = \{ P \in \Gamma \mid E_p(f_i) = E_{\widetilde{P}}(f_i) \}$$

定义在条件概率分布 $P(Y \mid X)$ 上的条件熵为：

$$H(P) = - \sum\limits_{x, y} \widetilde{P}(x)P(y \mid x) logP(y \mid x)$$

则模型集合 $M$ 中条件熵 $H(P)$ 最大的模型成为 **最大熵模型**

### 2.2.3 最大熵模型的学习

最大熵模型的学习过程就是模型的求最优解过程。所以学习过程可以转化成约束最优化过程

对于训练集 $T = \{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N) \} $ 以及特征函数 $ f_i(x, y)$，最大熵模型的学习等价于约束最优化问题：

$$ \max\limits_{P \in M} H(P) = - \sum\limits_{x,y} \widetilde{P}(x)P(y \mid x)logP(y \mid x)$$  

约束条件：

$$ E_P(f_i) = E_{\widetilde{P}}(f_i), i = 1, 2, ..., n$$

$$ \sum\limits_y P(y \mid x) = 1$$

按惯例转化为求最小值问题：

$$ \min\limits_{P \in M} - H(P) = \sum\limits_{x,y} \widetilde{P}(x)P(y \mid x)logP(y \mid x)$$

约束条件：

$$ E_P(f_i) - E_{\widetilde{P}}(f_i) = 0, i = 1, 2, ..., n$$

$$ \sum\limits_y P(y \mid x) = 1$$

上面式子的解就是最大熵模型学习的解，其解为：

$$ w* = arg\max\limits_w \Psi(w)$$

这里：

$$ \Psi(w) = \min\limits_{P \in M} L(P, w) = L(P_w, w)$$

$L$ 是拉格朗日函数：

$$ L(P, w) = -H(P) + w_0 \left( 1 - \sum\limits_yP(y \mid x) \right) + \sum\limits_{i=1}^n w_i (E_{\widetilde{P}}(f_i) - E_p(f_i))$$

---

# 3 导读笔记

## 3.1 逻辑回归模型

对数线性模型的形式为： $logP(y \mid x) = w \cdot x$

对于给定的 $x$，我们希望把它的类别概率表示为 $P(y = 1 \mid x) = \pi(x)$，其中 $\pi(x) \in [0, 1]$

为了用线性函数 $w \cdot x$ 来表示 $\pi(x)$ 的特征，需要一个变换，把 $(-\infty, +\infty) \to [0, 1]$，用到了 logit 变换：

$$ logit(\pi(x)) = log \dfrac{\pi(x)} {1 - \pi(x)} \in (-\infty, +\infty)$$

$$ log \dfrac{\pi(x)} {1 - \pi(x)} = w \cdot x$$

反解出逻辑回归模型：

$$ \pi(x) = \dfrac{e^{wx}}{1 + e^{wx}} = P(Y = 1 \mid x) $$

$$ \pi(x) = \dfrac{1}{1 + e^{wx}} = P(Y = 0 \mid x) $$

只需要求出 $w$ 就有了模型，然后对于新的 $x$，求出对应的 $P(Y=1 \mid x)$，当 $P(Y=1 \mid x) > 0.5$ 时，判定分类为 1

### 求解 w

用极大似然估计法求解 $w$：给定 w，求样本的联合概率密度，让它最大，即可求出 w

对于 $y_i \in \{0, 1\}$，给定 $\pi(x)$ 后，$y$ 的概率分布为：

$$ P_w(y \mid x) = \pi(x)^y [1 - \pi(x)]^{(1-y)} $$

对于 N 个样本的极大似然估计函数 $L(w)$：

$$L(w) = \prod\limits_{i=1}^N \pi(x)^{y_i} [1 - \pi(x)]^{(1-y_i)}$$

只要求 $\max logL(w)$ 的 $w$ 即可。一般用梯度下降法，给定一个初值 $w_0$ 即可找到 $w*$

## 3.2 最大熵模型

**原理：**在满足约束条件的模型集合中，选择熵最大的模型（混乱程度最大的模型）。因为在条件不足时，只能假设各种情况出现的概率相同

例如：对于 $X \in \{A, B, C, D, E\}$，估计各值出现的概率。

约束条件：

$$ \sum\limits_1^5 P(x_i) = 1 $$

$$P(x_i) \ge 0 $$

在满足约束的模型中找最乱的，所以各概率相同，所以有 $P(x_i) = 1/5$

对于新的约束条件：

$$ P(A) + P(B) = \dfrac{3}{10} $$

如何找集合中熵最大的模型

因为熵 $H(p) = - \sum p_i log p_i,\max H = \min [-H]$，所以：

要求的模型等价于求最优问题：

$$\min[-H(p)] = \sum\limits_{i=1}^5 P(y_i) \log P(y_i)$$

约束条件：

$$  \sum\limits_1^5 P(x_i) = 1  $$

$$P(x_i) \ge 0 $$

$$  P(A) + P(B) = \dfrac{3}{10}  $$

我们 $x$ 是已知的，如何把已知的 $x$ 信息加入来求 $p(y)$？

用条件熵：

$$ H(p) = -\sum\limits_{x, y} \widetilde{p}(x)p(y \mid x) \log P(y \mid x)$$

因为熵 $H(p) = -\sum p_i \log p_i$

$$H(y \mid x) = - \sum p(y \mid x) \log p(y \mid x)$$

所以：

$$ H(p) = E_x H(y \mid x) = \sum\limits_{x, y} p(x) p(y \mid x) \log p(y \mid x)$$

我们希望总体 $x$ 的分布用样本上的 $x$ 分布（也称经验分布）来代替，即

$$ \sum\limits_{x, y} p(x) p(y \mid x) \log p(y \mid x) = \sum\limits_{x, y} \widetilde{p}(x)p(y \mid x) \log P(y \mid x) $$

就要引入约束条件：

$$ E_{p(x, y)} f_i(x, y) = E_{\widetilde{p}(x, y)}f_i(x, y) $$

上式的 $p(x, y)$ 为联合概率分布，上式的含义是让每个特征在总体中出现的概率 = 样本中出现的概率

其中 $f_i(x, y)$ 是特征函数，代表观察到的信息

这样最大熵模型对应的最优化问题如下：

$$\max\limits_{p \in M} H(p) = - \sum\limits_{x, y} \widetilde{p}(x) p(y \mid x) \log p(y \mid x)$$

约束条件 n+1 个：

$$ E_p(f_i) = E_{\widetilde{p}}(f_i)$$

$$ \sum\limits_y p(y \mid x) = 1$$

其中：$i = 1, 2, ..., n$

最优问题的求解结果为：

$$ P_w(y \mid x) = \dfrac{1}{Z_w(x)} e^{\sum\limits_{i=1}^n w_if_i} $$

其中：

$$w = arg\max L_{\widetilde{p}} (P_w) = \log \pi(y \mid x)$$

求解的流程：

1. 给定一个 $x$，求 $y$ 取不同值时对应的概率分布
2. $(x, y)$ 满足 n 个 $f_i$ 的要求，$w_i$ 是特征 $f_i$ 的重要程度，当满足的特征越多且特征越重要，$P_w$ 值越大
3. 求 $w$，代入 $P_w(y \mid x)$ 就得到模型



