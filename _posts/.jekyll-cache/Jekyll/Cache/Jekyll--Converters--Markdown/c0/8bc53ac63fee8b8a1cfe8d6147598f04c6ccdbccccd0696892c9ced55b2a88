I"F3<h1 id="第-4-章-朴素贝叶斯法">第 4 章 朴素贝叶斯法</h1>

<h2 id="11-本章概要">1.1 本章概要</h2>

<ol>
  <li>
    <p>朴素贝叶斯法是生成学习方法</p>

    <p>利用训练数据学习 <span>$P(X|Y)$</span> 和 <span>$P(Y)$</span> 的估计，得到联合概率分布：</p>

    <script type="math/tex; mode=display">P(X, Y) = P(X|Y)P(Y)</script>
  </li>
  <li>
    <p>条件独立性</p>

    <script type="math/tex; mode=display">P(X = x | Y= c_k) = P(X^{(1)} = x^{(1)}, X^{(2)} = x^{(2)}, ..., X^{(n)} = x^{(n)} | Y = c_k) = \prod\limits_{j=1}^{n}P(X^{(j)} = x^{(j)} | Y =c_k)</script>

    <p>此假设使朴素贝叶斯的学习与预测大为简化，易于实现，但分类性能不一定高</p>
  </li>
  <li>
    <p>利用联合概率模型进行分类预测</p>

    <script type="math/tex; mode=display">P(Y|X) = \dfrac{P(X, Y)}{P(X)} = \dfrac{P(Y)P(X|Y)}{\sum\limits_YP(Y)P(X|Y)}</script>

    <p>将输入 <span>$x$</span> 分给后验概率最大的类 <span>$y$</span>：</p>

    <script type="math/tex; mode=display">y = arg\max\limits_{c_k} P(Y=c_k) \prod\limits_{j=1}^{n} P(X_j = x^{(j)} | Y = c_k)</script>
  </li>
  <li>
    <p>后验概率最大等价于 0-1 损失函数时的期望风险最小化</p>
  </li>
</ol>

<h2 id="12-目录">1.2 目录</h2>

<ol>
  <li>
    <p>朴素贝叶斯法的学习与分类</p>

    <p>1.1 基本方法</p>

    <p>1.2 后验概率最大化的含义</p>
  </li>
  <li>
    <p>朴素贝叶斯的参数估计</p>

    <p>2.1 极大似然估计</p>

    <p>2.2 学习与分类算法</p>

    <p>2.3 贝叶斯估计</p>
  </li>
</ol>

<hr />

<h2 id="21-朴素贝叶斯法的学习与分类">2.1 朴素贝叶斯法的学习与分类</h2>

<p>思路：</p>

<ol>
  <li>假设输入特征是条件独立的</li>
  <li>学习输入和输出的联合概率分布</li>
  <li>对于给定的输入，利用<strong>贝叶斯定理</strong>求出后验概率最大的输出</li>
</ol>

<h3 id="211-基本方法">2.1.1 基本方法</h3>

<ol>
  <li>
    <p>输入与输出</p>

    <p>输入特征向量 <span>$x \in \chi$<span> ，输出类标记 <span>$y \in \Upsilon$<span>，令 $X$ 是定义在 $\chi$ 上的随机变量，$Y$ 是定义在输出空间 $\Upsilon$ 上的随机变量，则 $P(X, Y)$ 是 $X$ 和 $Y$ 的联合概率分布</span></span></span></span></p>
  </li>
  <li>
    <p>训练数据集</p>

    <p><span>$ T = { (x_1, y_1), (x_2, y_2), …, (x_n, y_n) }$<span> 由 <span>$P(X, Y)$<span> 独立同分布产生</span></span></span></span></p>
  </li>
  <li>
    <p>通过训练数据集 <span>$T$<span> 学习联合概率分布 <span>$P(X, Y)$<span></span></span></span></span></p>

    <ul>
      <li>
        <p>学习先验概率分布</p>

        <script type="math/tex; mode=display">P(Y = c_k), k = 1, 2, ..., K</script>
      </li>
      <li>
        <p>学习条件概率分布</p>

        <script type="math/tex; mode=display">P(X = x| Y= c_k) = P(X^{(1)} = x^{(1)}, X^{(2)} = x^{(2)}, ..., X^{(n)} = x^{(n)} | Y = c_k)</script>

        <p><strong>强力假设</strong>：条件独立，朴素贝叶斯得名于此</p>

        <script type="math/tex; mode=display">P(X = x| Y = c_k) = \prod\limits_{j=1}^{n}P(X^{(j)} = x^{(j)} | Y =c_k)</script>
      </li>
      <li>
        <p>得到联合概率分布</p>

        <script type="math/tex; mode=display">P(X, Y) = P(X = x|Y= c_k)P(Y=c_k)</script>
      </li>
    </ul>
  </li>
  <li>
    <p>对新输入 <span>$x$</span> 分类，计算后验概率分布 <span>$P(Y=c_k|X=x)$</span></p>

    <script type="math/tex; mode=display">P(Y = c_k | X = x) = \dfrac{P(X=x|Y=c_k)P(Y=c_k)}{P(X)} = \dfrac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_kP(X=x|Y=c_k)P(Y=c_k)}</script>
  </li>
  <li>
    <p>朴素贝叶斯分类器</p>

    <script type="math/tex; mode=display">y = f(x) = arg\max\limits_{c_k} \dfrac{P(X=x|Y=c_k)P(Y=c_k)}{P(X)} = arg\max\limits_{c_k} \prod\limits_j P(X^{(j)} = x^{(j)} | Y =c_k)</script>
  </li>
</ol>

<h3 id="212-后验概率最大化的含义">2.1.2 后验概率最大化的含义</h3>

<p>等价于期望风险最小化</p>

<h2 id="22-朴素贝叶斯的参数估计">2.2 朴素贝叶斯的参数估计</h2>

<h3 id="221-极大似然估计">2.2.1 极大似然估计</h3>

<p>朴素贝叶斯的学习即从训练数据集 <span>$T$<span> 中估计 <span>$P(Y = c_k)$<span> 和 <span>$P(X^{(j)} = x^{(j)} | Y = c_k)$<span></span></span></span></span></span></span></p>

<ol>
  <li>
    <p>先验概率 <span>$P(Y = c_k)$<span> 的极大似然估计：</span></span></p>

    <script type="math/tex; mode=display">P(Y = c_k) = \dfrac{\sum\limits_{i=1}^N I(y_i = c_k)} {N} = \dfrac{\#\{y_i = c_k\}}{N}</script>

    <p>其中：<span>$ k = 1, 2, …, K $<span></span></span></p>
  </li>
  <li>
    <p>条件概率 <span>$P(X^{(j)} = x^{(j)} | Y = c_k)$<span></span></span></p>

    <p>假如第 <span>$j$<span> 个特征 <span>$x^{(j)}$<span> 可以取 <span>${a_{j1}, a_{j2}, … a_{jS_j} }$<span> 这些值，则</span></span></span></span></span></span></p>

    <script type="math/tex; mode=display">P(X^{(j)} = a_{jl} | Y = c_k) = \dfrac{ \#\{ X^{(j)} = a_{jl}, Y=c_k \} }{ \#\{ Y = c_k\}}</script>
  </li>
</ol>

<h3 id="222-学习与分类算法">2.2.2 学习与分类算法</h3>

<p>算法 4.1 (朴素贝叶斯算法)</p>

<ul>
  <li>
    <p>输入</p>

    <p>训练数据 <span>$ T = { (x_1, y_1), (x_2, y_2), …, (x_n, y_n) }$<span>，</span></span></p>

    <p>其中:</p>

    <p><span>$x_i = (x^{(1)}_i, x^{(2)}_i, …, x^{(n)}_i)^T$</span> ，<span>$x^{(j)}_i$</span> 是第 <span>$i$</span> 个样本的第 <span>$j$</span> 个特征</p>

    <p><span>$x^{(j)}<em>i \in { a</em>{j1}, a_{j2}, …, a_{jS_j} }$</span>， <span>$a_{jl}$</span> 是第 <span>$j$</span> 个特征的第 <span>$l$</span> 个取值</p>
  </li>
  <li>
    <p>输出</p>

    <p>实例 $x$ 的分类</p>
  </li>
  <li>
    <p>过程</p>

    <ol>
      <li>
        <p>计算先验概率 $P(Y = c_k)$</p>

        <script type="math/tex; mode=display">P(Y = c_k) = \dfrac{\sum\limits_{i=1}^N I(y_i = c_k)} {N} = \dfrac{\#\{y_i = c_k\}}{N}</script>
      </li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>计算条件概率 &lt;/span&gt;$ P(X^{(j)} = x^{(j)}</td>
              <td>Y = c_k) $&lt;/span&gt;</td>
            </tr>
          </tbody>
        </table>

        <script type="math/tex; mode=display">P(X^{(j)} = a_{jl} | Y = c_k) = \dfrac{ \#\{ X^{(j)} = a_{jl}, Y=c_k \} }{ \#\{ Y = c_k\}}</script>
      </li>
      <li>
        <p>对于实例 <span>$x$</span> 计算</p>

        <script type="math/tex; mode=display">P(Y=c_k)\prod\limits_j P(X^{(j)} = x^{(j)} | Y =c_k)</script>
      </li>
      <li>
        <p>确定实例 $x$ 的类</p>

        <script type="math/tex; mode=display">arg\max\limits_{c_k} \prod\limits_j P(X^{(j)} = x^{(j)} | Y =c_k)</script>
      </li>
    </ol>
  </li>
</ul>

<h3 id="223-贝叶斯估计">2.2.3 贝叶斯估计</h3>

<p>当训练数据集 $T$ 数量少，但类别多时，可能某个类别没有收集到实例，这地极大似然估计会遇到要估计的概率值为0的情况。这时采用贝叶斯估计来处理。</p>

<ol>
  <li>
    <p>条件概率 <span>$P(X^{(j)} = x^{(j)} | Y = c_k)$</span> 的贝叶斯估计为</p>

    <script type="math/tex; mode=display">P(X^{(j)} = a_{jl} | Y = c_k) = \dfrac{ \#\{ X^{(j)} = a_{jl}, Y=c_k \} + \lambda}{ \#\{ Y = c_k\} + S_j \lambda}</script>

    <p>其中 $S_j$ 是第 $j$ 个特征可能取值的总数</p>
  </li>
  <li>
    <p>先验概率 $P(Y = c_k)$ 的贝叶斯估计</p>

    <script type="math/tex; mode=display">P(Y = c_k) = \dfrac{\sum\limits_{i=1}^N I(y_i = c_k)+\lambda} {N+K\lambda} = \dfrac{\#\{y_i = c_k\} + \lambda}{N + K\lambda}</script>
  </li>
</ol>

<hr />

<h1 id="第-5-章-决策树">第 5 章 决策树</h1>

<p>决策树是基本的分类和回归方法。在分类问题中，基于特征对实例进行分类，相当于 if-else 规则的集合。主要优点是模型具有可读性、分类速度快。学习通常包括3个步骤：特征选择、决策树的生成和决策树的修剪本意介绍三个算法：ID3、C4.5 和 CART 算法</p>

<h2 id="11-本章概要-1">1.1 本章概要</h2>

<ol>
  <li>
    <p>决策树的学习，旨在构建一个拟合训练数据，且复杂度小的决策树。现实中采用启发式方法学习</p>
  </li>
  <li>
    <p>决策树学习算法包括 3 个部分：特征选择、树的生成和树的剪枝。常用的方法有：ID3、C4.5 和 CART</p>
  </li>
  <li>
    <p>特征选择目标在于选择给训练数据 $T$ 分类的特征。常用准则如下：</p>

    <ul>
      <li>
        <p>信息增益（ID3）</p>

        <p>样本集合 D 对特征 A 的 信息增益</p>

        <script type="math/tex; mode=display">g(D, A) = H(D) - H(D|A)</script>

        <ul>
          <li>
            <p>数据集 D 的熵：<span>$ H(D) = - \sum\limits_{k=1}^K \dfrac{|C_k|}{|D|} log_2 \dfrac{|C_k|}{|D|}$<span></span></span></p>
          </li>
          <li>
            <p>给定特征 A 时数据集 D 的条件熵：<span>$ H(D|A) = \sum\limits_{i=1}^{n} \dfrac{|D_i|}{|D|} H(D_i)$<span></span></span></p>
          </li>
          <li>
            <p><span>$D_i$<span>  是 D 中特征 A 取第 $i$ 个值时的样本子集</span></span></p>
          </li>
          <li><span>$C_k$<span> 是 D 中属于第 <span>$k$<span> 类的样本子集</span></span></span></span></li>
          <li><span>$n$<span> 是特征 A 的取值个数</span></span></li>
          <li><span>$K$<span> 是类的个数</span></span></li>
        </ul>
      </li>
      <li>
        <p>信息增益率（C4.5）</p>

        <script type="math/tex; mode=display">g_R(D, A) = \dfrac{g(D, a)}{H_A(D)}</script>

        <ul>
          <li><span>$H_A(D)$<span> 是 D 关于特征 A 的值的熵</span></span></li>
        </ul>
      </li>
      <li>
        <p>基尼指数（CART）</p>

        <script type="math/tex; mode=display">Gini(D) = 1 - \sum\limits_{k=1}{K} \left( \dfrac{|C_k|}{|D|}\right)^2</script>

        <p>特征 A 下集合 D 的基尼指数：</p>

        <script type="math/tex; mode=display">Gini\_index(D, A) = \sum\limits_{v=1}^{V} \dfrac{|D^v|}{|D|} Gini(D^v)</script>

        <blockquote>
          <p>上式摘自《西瓜书》</p>
        </blockquote>
      </li>
    </ul>
  </li>
  <li>
    <p>决策树的生成</p>

    <p>以信息增益最大、信息增益率最大或基尼指数最小为准则，从根结点开始，不断选择最优特征，用特征分割当前集合</p>
  </li>
  <li>
    <p>决策树的剪枝</p>

    <p>解决过拟合问题。从已生成的树上剪掉一些叶结点或子树，将其父结点设为新的叶结点</p>
  </li>
</ol>

<h2 id="12-本章目录">1.2 本章目录</h2>

<ol>
  <li>
    <p>决策树模型与学习</p>

    <p>1.1 决策树模型</p>

    <p>1.2 决策树与 if-then 规则</p>

    <p>1.3 决策树与条件概率分布</p>

    <p>1.4 决策树学习</p>
  </li>
  <li>
    <p>特征选择</p>

    <p>2.1 特征选择问题</p>

    <p>2.2 信息增益</p>

    <p>2.3 信息增益率</p>
  </li>
  <li>
    <p>决策树的生成</p>

    <p>3.1 ID3 算法</p>

    <p>3.2 C4.5 算法</p>
  </li>
  <li>
    <p>决策树的剪枝</p>
  </li>
  <li>
    <p>CART 算法</p>

    <p>5.1 CART 生成</p>

    <p>5.2 CART 剪枝</p>
  </li>
</ol>

<hr />

<h2 id="21-决策树模型与学习">2.1 决策树模型与学习</h2>

<h3 id="211-模型">2.1.1 模型</h3>

<p>决策树由<strong>结点</strong>和<strong>有向边</strong>组成，结点分成<strong>内结点</strong>和<strong>叶结点</strong>，内结点表一个特征或属性，叶结点表类别</p>

<p>分类的过程：从根结点开始，根据实例 $x$ 的某个特征将 $x$ 分配到根结点下一级的子结点，下一级的每个子结点对应着此特征的一个取值。继续考察 $x$ 剩余的特征，直到将 $x$ 分配到叶结点中，此叶结点的类别就是 $x$ 的类别</p>

<h3 id="222-学习">2.2.2 学习</h3>

<ol>
  <li>
    <p>学习的目标</p>

    <p>通过训练集构建一套决策树模型，能对实例进行正确的分类。本质是从训练集中归纳出一组分类规则，既适应训练集又具有泛化能力。</p>
  </li>
  <li>
    <p>学习策略</p>

    <p>我们用损失函数表示目标，学习策略（方案）是以损失函数为目标函数的最小化</p>
  </li>
  <li>
    <p>学习算法</p>

    <p>学习算法是递归选择最优特征，用特征对当前数据进行分割，使子数据集有一个最好的分类</p>
  </li>
  <li>
    <p>剪枝</p>

    <p>对生成的树从下而上剪枝，将树变简单，去年过于细分的叶结点</p>
  </li>
</ol>

<h2 id="22-特征选择">2.2 特征选择</h2>

<h3 id="221-特征选择问题">2.2.1 特征选择问题</h3>

<p>若一个特征分类结果与随机分类结果没有差异，这个特征无分类能力。分类的好坏用信息增益或信息增益率来衡量</p>

<h4 id="2211-信息增益">2.2.1.1 信息增益</h4>

:ET