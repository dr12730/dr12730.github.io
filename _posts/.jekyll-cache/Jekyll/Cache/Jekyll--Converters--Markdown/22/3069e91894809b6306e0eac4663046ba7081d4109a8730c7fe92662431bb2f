I"<!--more-->

<p>本文是 <a href="https://arxiv.org/abs/1704.06857">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a> 论文的翻译</p>

<h1 id="摘要">摘要</h1>

<p>计算机视觉与机器学习研究者对图像语义分割问题越来越感兴趣。越来越多的应用场景需要精确且高效的分割技术，如自动驾驶、室内导航、甚至虚拟现实与增强现实等。这个需求与视觉相关的各个领域及应用场景下的深度学习技术的发展相符合，包括语义分割及场景理解等。这篇论文回顾了各种应用场景下利用深度学习技术解决语义分割问题的情况：首先，我们引入了领域相关的术语及必要的背景知识；然后，我们介绍了主要的数据集以及对应的挑战，帮助研究者选取真正适合他们问题需要及目标的数据集；接下来，我们介绍了现有的方法，突出了各自的贡献以及对本领域的积极影响；最后，我们展示了大量的针对所述方法及数据集的实验结果，同时对其进行了分析；我们还指出了一系列的未来工作的发展方向，并给出了我们对于目前最优的应用深度学习技术解决语义分割问题的研究结论。</p>

<h1 id="1-引言">1 引言</h1>

<p>如今，语义分割（应用于静态 2D 图像、视频甚至 3D 数据、体数据）是计算机视觉的关键问题之一。在宏观意义上来说，语义分割是为场景理解铺平了道路的一种高层任务。作为计算机视觉的核心问题，场景理解的重要性越来越突出，因为现实中越来越多的应用场景需要从影像中推理出相关的知识或语义（即由具体到抽象的过程）。这些应用包括自动驾驶[1,2,3]，人机交互[4]，计算摄影学[5]，图像搜索引擎[6]，增强现实等。应用各种传统的计算机视觉和机器学习技术，这些问题已经得到了解决。虽然这些方法很流行，但深度学习革命让相关领域发生了翻天覆地的变化，因此，包括语义分割在内的许多计算机视觉问题都开始使用深度架构来解决，通常是卷积神经网络 CNN[7-11]，而 CNN 在准确率甚至效率上都远远超过了传统方法。然而，相比于固有的计算机视觉及机器学习分支，深度学习还远不成熟。也因此，还没有一个统一的工作及对于目前最优方法的综述。该领域的飞速发展使得对初学者的启蒙教育比较困难，而且，由于大量的工作相继被提出，要跟上发展的步伐也非常耗时。于是，追随语义分割相关工作、合理地解释它们的论点、过滤掉低水平的工作以及验证相关实验结果等是非常困难的。</p>

<p>就我所知，本文是第一篇致力于综述用于语义分割的深度模型技术的文章。已经有较多的关于语义分割的综述调查，比如[12,13]等，这些工作在总结、分类现有方法、讨论数据集及评价指标以及为未来研究者提供设计思路等方面做了很好的工作。但是，这些文章缺少对某些最新数据集的介绍，他们不去分析框架的情况，而且没有提供深度学习技术的细节。因此，我们认为本文是全新的工作，而且这对于深度学习相关的语义分割社区有着重要意义。</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/pic1.jpg" alt="pic1" /></p>

<p>图 1 物体识别或场景理解相关技术从粗粒度推理到细粒度推理的演变：四幅图片分别代表分类、识别与定位、语义分割、实例分割。</p>

<p>本文核心贡献如下：</p>

<ol>
  <li>我们对于现有的数据集给出了一个全面的调查，这些数据集将会对深度学习技术推动的分割项目发挥作用；</li>
  <li>我们对于多数重要的深度学习语义分割方法进行了深度有条理的综述，包括他们的起源、贡献等；</li>
  <li>我们进行了彻底的性能评估，使用了多种评价指标如准确率、运行时间、内存占用等；</li>
  <li>我们对以上结果进行了讨论，并给出了未来工作的一系列可能的发展方向，这些方向可能在未来的发展进程中取得优势。我们还给出了该领域目前最好方法的总结。</li>
</ol>

<p>本文剩余部分安排：</p>

<ul>
  <li>第二章介绍了语义分割问题，同时引入了相关工作中常用的符号、惯例等。其他的背景概念如通用的深度神经网络也在这章中回顾；</li>
  <li>第三章介绍了现有的数据集、挑战及实验基准；</li>
  <li>第四章回顾了现有方法，基于其贡献自下而上排序。本章重点关注这些方法的理论及闪光点，而不是给出一个定量的评估；</li>
  <li>第五章给出了一个简短的对于现有方法在给定数据集上定量表现的讨论，另外还有未来相关工作的发展方向；</li>
  <li>第六章则总结全文并对相关工作及该领域目前最优方法进行了总结。</li>
</ul>

<h1 id="2-术语及背景概念">2 术语及背景概念</h1>

<p>为了更好地理解语义分割问题是如何用深度学习框架解决的，有必要了解到其实基于深度学习的语义分割并不是一个孤立的领域，而是在从粗糙推理到精细化推理过程中很自然的一步。这可以追溯到分类问题，包括对整个输入做出预测，即预测哪个物体是属于这幅图像的，或者给出多个物体可能性的排序。对于细粒度推理来说，将接下来进行物体的定位与检测，这将不止提供物体的类别，而且提供关于各类别空间位置的额外信息，比如中心点或者边框。这样很显然，语义分割是实现细粒度推理的很自然的一步，它的目标是：对每个像素点进行密集的预测，这样每个像素点均被标注上期对应物体或区域的类别。这还可以进一步改进，比如实例分割（即对同一类的不同实例标以不同的标签），甚至是基于部分的分割（即对已经分出不同类别的图像进行底层分解，找到每个类对应的组成成分）。图 1 展示了以上提到的演变过程。在本文中，我们主要关注一般的场景标注，也就是像素级别的分割，但是我们也会回顾实例分割及基于部分的分割的较重要的方法。</p>

<p>最后，像素级别的标注问题可以松弛为以下公式：对于随机变量集合 $X = {x_1, x_2, …, x_n}$  中的每个随机变量，找到一种方法为其指派一个来自标签空间 $L = {l_1, l_2, …, l_k$ 中的一个状态。每个标签 $l$ 表示唯一的一个类或者物体，比如飞机、汽车、交通标志或背景等。这个标签空间有$k$ 个可能的状态，通常会被扩展为 $k+1$ 个，视 $l_0$ 为背景或者空的类。通常，$X$ 是一个二维的图像，包含 $W \times H=N$ 的像素点 $x$。但是，这个随机变量的集合可以被扩展到任意维度，比如体数据或者超谱图像。</p>

<p>除了问题的定义，回顾一些可能帮助读者理解的背景概念也是必要的。首先是一些常见的被用作深度语义分割系统的网络、方法以及设计决策；另外还有用于训练的一些常见的技术比如迁移学习等。最后是数据的预处理以及增强式的方法等。</p>

<h2 id="21-常见的深度网络架构">2.1 常见的深度网络架构</h2>

<p>正如之前所讲，某些深度网络已经对该领域产生了巨大的贡献，并已成为众所周知的领域标准。这些方法包括 AlexNet，VGG-16，GoogLeNet，以及 ResNet。还有一些是由于其被用作许多分割架构的一部分而显得重要。因此，本文将在本章致力于对其进行回顾。</p>

<h3 id="211-alexnet">2.1.1 AlexNet</h3>

<p>AlexNet（以作者名字 Alex 命名）首创了深度卷积神经网络模型，在 2012 年 ILSVRC（ImageNet 大规模图像识别）竞赛上以 top-5 准确率 84.6%的成绩获胜，而与之最接近的竞争者使用了传统的而非深度的模型技术，在相同的问题下仅取得了 73.8%的准确率。由 Krizhecsky 等人[14]给出的架构相对简单，包括卷积层、max-pooling 层及 ReLU 层各五层作为非线性层，全连接层三层以及 dropout 层。图 2 给出了这个架构的示意。</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/pic2.png" alt="pic2" /></p>

<p>图 2 文献[14]中给出的 AlexNet 卷积神经网络架构</p>

<h3 id="212-vgg">2.1.2 VGG</h3>

<p>VGG 是由牛津大学 Visual Geometry Group 提出的卷积神经网络模型（以课题组的名字命名）。他们提出了深度卷积神经网络的多种模型及配置[15]，其中一种提交到了 2013 年 ILSVRC（ImageNet 大规模图像识别）竞赛上。这个模型由于由 16 个权重层组成，因此也被称为 VGG-16，其在该竞赛中取得了 top-5 上 92.7%的准确率。图 3 展示了 VGG-16 的模型配置。VGG-16 与之前的模型的主要的不同之处在于，其在第一层使用了一堆小感受野的卷积层，而不是少数的大感受野的卷积层。这使得模型的参数更少，非线性性更强，也因此使得决策函数更具区分度，模型更好训练。</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/pic3.png" alt="pic3" /></p>

<p>图 3  VGG-16卷积神经网络模型架构，本图经许可取自Matthieu Cord的演讲。</p>

<h3 id="213-googlenet">2.1.3 GoogLeNet</h3>

<p>GoogLeNet是由Szegedy等人[16]提出的在ILSVRC-2014竞赛上取得top-5上93.3%准确率的模型。这个CNN模型以其复杂程度著称，事实上，其具有22个层以及新引入的inception模块（如图4所示）。这种新的方法证实了CNN层可以有更多的堆叠方式，而不仅仅是标准的序列方式。实际上，这些模块由一个网络内部的网络层（NiN）、一个池化操作、一个大卷积核的卷积层及一个小核的卷积层组成。所有操作均并行计算出来，而后进行1×1卷积操作来进行降维。由于这些模块的作用，参数及操作的数量大大减少，网络在存储空间占用及耗时等方面均取得了进步</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/pic4.png" alt="pic4" /></p>

<p>图 4 GoogLeNet框架中带有降维的Inception模块。</p>

<h3 id="214-resnet">2.1.4 ResNet</h3>

<p>微软提出的ResNet[17]由于在ILSVRC-2016中取得的96.4%的准确率而广受关注。除了准确率较高之外，ResNet网络还以其高达152层的深度以及对残差模块的引入而闻名。残差模块解决了训练真正深层网络时存在的问题，通过引入identity skip connections网络各层可以把其输入复制到后面的层上。</p>

<p>本方法的关键想法便是，保证下一层可以从输入中学到与已经学到的信息不同的新东西（因为下一层同时得到了前一层的输出以及原始的输入）。另外，这种连接也协助解决了梯度消失的问题。</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/pic5.png" alt="pic5" /></p>

<p>图 5 ResNet中的残差模块。</p>

<h3 id="215-renet">2.1.5 ReNet</h3>

<p>为了将循环神经网络RNN模型扩展到多维度的任务上，Graves等人[18]提出了一种多维度循环神经网络（MDRNN）模型，将每个单一的循环连接替换为带有d个连接的标准RNN，其中d是数据的spatio-temporal维度。基于这篇工作，Visin等人[19]提出了ReNet模型，其不使用多维RNN模型，而是使用常见的序列RNN模型。这样，RNN模型的数量在每一层关于d（输入图像的维数2d）线性增长。在ReNet中，每个卷积层（卷积+池化）被4个同时在水平方向与竖直方向切分图像的RNN模型所替代，如图6所示：</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/pic6.png" alt="pic6" /></p>

<p>图 6 ReNet架构中的一层，对竖直与水平方向的空间依赖性建模</p>

<h2 id="22-迁移学习">2.2 迁移学习</h2>

<p>从头训练一个深度神经网络通常是不可行的，有这样两个原因：训练需要足量的数据集，而这一般是很难得到的；网络达到收敛需要很长的时间。即便得到了足够大的数据集并且网络可以在短时间内达到收敛，从之前的训练结果中的权重开始训练也总比从随机初始化的权重开始训练要好[20,21]。迁移学习的一种重要的做法便是从之前训练好的网络开始继续训练过程来微调模型的权重值。</p>

<p>Yosinski等人[22]证明了即便是从较不相关的任务中迁移学习来的特征也要比直接从随机初始化学习的特征要好，这个结论也考虑到了随着提前训练的任务与目标任务之间差异的增大，可迁移性将减小的情况。</p>

<p>然而，迁移学习技术的应用并没有如此的直接。一方面，使用提前训练的网络必须满足网络架构等的约束，不过，因为一般不会新提出一个全新的网络结构来使用，所以使用现有的网络架构或网络组件进行迁移学习是常见的；另一方面，迁移学习中的训练过程本身相对于从头开始的训练过程来说区别非常小。合理选择进行微调的层是很重要的，一般选网络中较高的层因为底层一般倾向于保留更加通用的特征；同时，合理地确定学习率也是重要的，一般选取较小的值，因为一般认为提前训练的权重相对比较好，无需过度修改。</p>

<p>由于收集和创建像素级别的分割标注数据集的内在的困难性，这些数据集的规模一般不如分类数据集如ImageNet[23,24]等的大。分割研究中数据集的规模问题在处理RGB-D或3D数据集时更加严重，因为这些数据集规模更小。也因此，迁移学习，尤其是从提前训练好的分类网络中微调而来的方式，将会成为分割领域的大势所趋，并且已经有方法成功地进行了应用，我们将在后面几章进行回顾。</p>

<h2 id="23-数据预处理与数据增强">2.3 数据预处理与数据增强</h2>

<p>数据增强技术被证明了有利于通用的尤其是深度的机器学习架构的训练，无论是加速收敛过程还是作为一个正则项，这也避免了过拟合并增强了模型泛化能力[15]。</p>

<p>数据增强一般包括在数据空间或特征空间（或二者均有）上应用一系列的迁移技术。在数据空间上应用增强技术最常见，这种增强技术应用迁移方法从已有数据中得到新的样本。有很多的可用的迁移方法：平移、旋转、扭曲、缩放、颜色空间转换、裁剪等。这些方法的目标均是通过生成更多的样本来构建更大的数据集，防止过拟合以及对模型进行正则化，还可以对该数据集的各个类的大小进行平衡，甚至手工地产生对当前任务或应用场景更加具有代表性的新样本。</p>

<p>数据增强对小数据集尤其有用，而且其效用已经在长期使用过程中被证明。例如，在[26]中，有1500张肖像图片的数据集通过设计4个新的尺寸（0.6,0.8,1.2,1.5），4个新的旋角（-45，-22,22,45），以及4个新的gamma变化（0.5,0.8,1.2,1.5）被增强为有着19000张训练图像的数据集。通过这一处理，当使用增强数据集进行微调时，其肖像画分割系统的交叠准确率（IoU）从73.09%提升到了94.20%。</p>

<h1 id="3-数据集及竞赛">3 数据集及竞赛</h1>

<p>以下两种读者应该阅读本部分内容：一是刚刚开始研究本领域问题的读者，再就是已经很有经验但是想了解最近几年其他研究者研究成果的可取之处的读者。虽然第二种读者一般很明确对于开始语义分割相关的研究来说数据集及竞赛是很重要的两个方面，但是对于初学者来说掌握目前最优的数据集以及（主流的）竞赛是很关键的。因此，本章的目标便是对研究者进行启发，提供一个对数据集的简要总结，这里面可能有正好他们需求的数据集以及数据增强或预处理等方面的技巧。不过，这也可以帮助到已经有深入研究的工作者，他们可能想要回顾基础或者挖掘新的信息。</p>

<p>值得争辩的是，对于机器学习来说数据是最重要的或者最重要的之一。当处理深度网络时，这种重要性更加明显。因此，收集正确的数据放入数据集对于任何基于深度学习的分割系统来说都是极为重要的。收集与创建一个足够大而且能够正确代表系统应用场景的数据集，需要大量的时间，需要领域专门知识来挑选相关信息，也需要相关的基础设施使得系统可以正确的理解与学习（捕捉到的数据）。这个任务的公式化过程虽然相比复杂的神经网络结构的定义要简单，但是其解决过程却是相关工作中最难的之一。因此，最明智的做法通常是使用一个现存的足够可以代表该问题应用场景的标准数据集。使用标准数据集还有一个好处就是可以使系统间的对比更加公平，实际上，许多数据集是为了与其他方法进行对比而不是给研究者测试其算法的，在对比过程中，会根据方法的实际表现得到一个公平的排序，其中不涉及任何数据随机选取的过程。</p>

<p>接下来我们将介绍语义分割领域最近最受欢迎的大规模数据集。所有列出的数据集均包含像素级别或点级别的标签。这个列表将根据数据内在属性分为3个部分：2维的或平面的RGB数据集，2.5维或带有深度信息的RGB（RGB-D）数据集，以及纯体数据或3维数据集。表1给出了这些数据集的概览，收录了所有本文涉及的数据集并提供了一些有用信息如他们的被构建的目的、类数、数据格式以及训练集、验证集、测试集划分情况。</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/tab1.png" alt="tab1" /></p>

<p>表 1  常见的大规模分割数据集</p>

<h2 id="31--2维数据集">3.1  2维数据集</h2>

<p>自始至终，语义分割问题最关注的是二维图像。因此，二维数据集在所有类型中是最丰富的。本章我们讨论语义分割领域最流行的二维大规模数据集，这考虑到所有的包含二维表示如灰度或RGB图像的数据集。</p>

<p>PASCAL视觉物体分类数据集<a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/">PASCAL-VOC</a> : 包括一个标注了的图像数据集和五个不同的竞赛：分类、检测、分割、动作分类、人物布局。分割的竞赛很有趣：他的目标是为测试集里的每幅图像的每个像素预测其所属的物体类别。有21个类，包括轮子、房子、动物以及其他的：飞机、自行车、船、公共汽车、轿车、摩托车、火车、瓶子、椅子、餐桌、盆栽、沙发、显示器（或电视）、鸟、猫、狗、马、绵羊、人。如果某像素不属于任何类，那么背景也会考虑作为其标签。该数据集被分为两个子集：训练集1464张图像以及验证集1449张图像。测试集在竞赛中是私密的。争议的说，这个数据集是目前最受欢迎的语义分割数据集，因此很多相关领域卓越的工作将其方法提交到该数据集的评估服务器上，在其测试集上测试其方法的性能。方法可以只用该数据集训练，也可以借助其他的信息。另外，其方法排行榜是公开的而且可以在线查询。</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/pic7.png" alt="pic7" /></p>

<p>PASCAL 上下文数据集<a href="http://www.cs.stanford.edu/∼roozbeh/pascal-context/">PASCAL Context</a>：对于PASCAL-VOC 2010识别竞赛的扩展，包含了对所有训练图像的像素级别的标注。共有540个类，包括原有的20个类及由PASCAL VOC分割数据集得来的图片背景，分为三大类，分别是物体、材料以及混合物。虽然种类繁多，但是只有59个常见类是较有意义的。由于其类别服从一个幂律分布，其中有很多类对于整个数据集来说是非常稀疏的。就这点而言，包含这59类的子集常被选作真实类别来对该数据集进行研究，其他类别一律重标为背景。</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/pic8.png" alt="pic8" /></p>

<p>PASCAL 部分数据集（PASCAL Part）[29] （http://www.stat.ucla.edu/∼xianjie.chen/pascal part dataset/pascal part.html）：对于PASCAL-VOC 2010识别竞赛的扩展，超越了这次竞赛的任务要求而为图像中的每个物体的部分提供了一个像素级别的分割标注（或者当物体没有连续的部分的时候，至少是提供了一个轮廓的标注）。原来的PASCAL-VOC中的类被保留，但被细分了，如自行车被细分为后轮、链轮、前轮、手把、前灯、鞍座等。本数据集包含了PASCAL VOC的所有训练图像、验证图像以及9637张测试图像的标签。</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/pic9.png" alt="pic9" /></p>

<p>语义边界数据集（SBD）[30] （http://home.bharathh.info/home/sbd）：是PASCAL数据集的扩展，提供VOC中未标注图像的语义分割标注。提供PASCAL VOC 2011 数据集中11355张数据集的标注，这些标注除了有每个物体的边界信息外，还有类别级别及实例级别的信息。由于这些图像是从完整的PASCAL VOC竞赛中得到的，而不仅仅是其中的分割数据集，故训练集与验证集的划分是不同的。实际上，SBD有着其独特的训练集与验证集的划分方式，即训练集8498张，验证集2857张。由于其训练数据的增多，深度学习实践中常常用SBD数据集来取代PASCAL VOC数据集。</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/pic10.png" alt="pic10" /></p>

<p>微软常见物体环境数据集（Microsoft COCO） [31]：(http://mscoco.org/) 是另一个大规模的图像识别、分割、标注数据集。它可以用于多种竞赛，与本领域最相关的是检测部分，因为其一部分是致力于解决分割问题的。该竞赛包含了超过80个类别，提供了超过82783张训练图片，40504张验证图片，以及超过80000张测试图片。特别地，其测试集分为4个不同的子集各20000张：test-dev是用于额外的验证及调试，test-standard是默认的测试数据，用来与其他最优的方法进行对比，test-challenge是竞赛专用，提交到评估服务器上得出评估结果，test-reserve用于避免竞赛过程中的过拟合现象（当一个方法有嫌疑提交过多次或者有嫌疑使用测试数据训练时，其在该部分子集上的测试结果将会被拿来作比较）。由于其规模巨大，目前已非常常用，对领域发展很重要。实际上，该竞赛的结果每年都会在ECCV的研讨会上与ImageNet数据集的结果一起公布。</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/pic11.png" alt="pic11" /></p>

<p>图像与注释合成数据集（SYNTHIA）[32] （http://synthia-dataset.net/）是一个大规模的虚拟城市的真实感渲染图数据集，带有语义分割信息，是为了在自动驾驶或城市场景规划等研究领域中的场景理解而提出的。提供了11个类别物体（分别为空、天空、建筑、道路、人行道、栅栏、植被、杆、车、信号标志、行人、骑自行车的人）细粒度的像素级别的标注。包含从渲染的视频流中提取出的13407张训练图像，该数据集也以其多变性而著称，包括场景（城镇、城市、高速公路等）、物体、季节、天气等。</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/pic12.png" alt="pic12" /></p>

<p>城市风光数据集 [33] （https://www.cityscapes-dataset.com/）是一个大规模的关注于城市街道场景理解的数据集，提供了8种30个类别的语义级别、实例级别以及密集像素标注（包括平坦表面、人、车辆、建筑、物体、自然、天空、空）。该数据集包括约5000张精细标注的图片，20000张粗略标注的图片。数据是从50个城市中持续数月采集而来，涵盖不同的时间以及好的天气情况。开始起以视频形式存储，因此该数据集按照以下特点手动选出视频的帧：大量的动态物体，变化的场景布局以及变化的背景。</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/pic13.png" alt="pic13" /></p>

<p>CamVid数据集 [55,34] （http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/）是一个道路、驾驶场景理解数据集，开始是五个视频序列，来自一个安装在汽车仪表盘上的960x720分辨率的摄相机。这些序列中采样出了701个帧（其中4个序列在1fps处，1个序列在15fps处），这些静态图被手工标注上32个类别：空、建筑、墙、树、植被、栅栏、人行道、停车场、柱或杆、锥形交通标志、桥、标志、各种文本、信号灯、天空、……（还有很多）。值得注意的是，Sturgess等人[35]将数据集按照367-100-233的比例分为训练集、验证集、测试集，这种分法使用了部分类标签：建筑、树、天空、车辆、信号、道路、行人、栅栏、杆、人行道、骑行者。</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/pic14.png" alt="pic14" /></p>

<p>KITTI [56] 是用于移动机器人及自动驾驶研究的最受欢迎的数据集之一，包含了由多种形式的传感器得出的数小时的交通场景数据，包括高分辨率RGB、灰度立体摄像机以及三维激光扫描器。尽管很受欢迎，该数据集本身并没有包含真实语义分割标注，但是，众多的研究者手工地为该数据集的部分数据添加标注以满足其问题的需求。Alvarez等人[36,37]为道路检测竞赛中的323张图片生成了真实标注，包含三个类别：道路、垂直面和天空。Zhang等人[39]标注了252张图片，其中140张训练、112张测试，其选自追踪竞赛中的RGB和Velodyne扫描数据，共十个类。Ros等人[38]在视觉测距数据集中标注了170个训练图片和46个测试图片，共11个类。</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/pic15.png" alt="pic15" /></p>

<p>YouTube物体数据集 [57] 是从YouTube上采集的视频数据集，包含有PASCAL VOC中的10个类。该数据集不包含像素级别的标注，但是Jain等人[42]手动的标注了其126个序列的子集。其在这些序列中每10个帧选取一张图片生成器语义标签，总共10167张标注的帧，每帧480x360的分辨率。</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/pic16.png" alt="pic16" /></p>

<p>Adobe肖像分割数据集 [26] （http://xiaoyongshen.me/webpage portrait/index.html） 包含从Flickr中收集的800x600的肖像照片，主要是来自手机前置摄像头。该数据集包含1500张训练图片和300张预留的测试图片，这些图片均完全被二值化标注为人或背景。图片被半自动化的标注：首先在每幅图片上运行一个人脸检测器，将图片变为600x800的分辨率，然后，使用Photoshop快速选择工具将人脸手工标注。这个数据集意义重大，因为其专门适用于人脸前景的分割问题。</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/pic17.png" alt="pic17" /></p>

<p>上下文语料数据集（MINC）[43] 是用于对块进行分类以及对整个场景进行分割的数据集。该数据集提供了23个类的分割标注（文中有详细的各个类别的名称），包含7061张标注了的分割图片作为训练集，5000张的测试集和2500张的验证集。这些图片均来自OpenSurfaces数据集[58]，同时使用其他来源如Flickr或Houzz进行增强。因此，该数据集中的图像的分辨率是变化的，平均来看，图片的分辨率一般是800x500或500x800。</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/pic18.png" alt="pic18" /></p>

<p>密集标注的视频分割数据集（DAVIS）[44,45]（http://davischallenge.org/index.html）：该竞赛的目标是视频中的物体的分割，这个数据集由50个高清晰度的序列组成，选出4219帧用于训练，2023张用于验证。序列中的帧的分辨率是变化的，但是均被降采样为480p的。给出了四个不同类别的像素级别的标注，分别是人、动物、车辆、物体。该数据集的另一个特点是每个序列均有至少一个目标前景物体。另外，该数据集特意地较少不同的大动作物体的数量。对于那些确实有多个前景物体的场景，该数据集为每个物体提供了单独的真实标注，以此来支持实例分割。</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/pic19.png" alt="pic19" /></p>

<p>斯坦福背景数据集[40] （http://dags.stanford.edu/data/iccv09Data.tar.gz）包含了从现有公开数据集中采集的户外场景图片，包括LabelMe, MSRC, PASCAL VOC 和Geometric Context。该数据集有715张图片（320x240分辨率），至少包含一个前景物体，且有图像的水平位置信息。该数据集被以像素级别标注（水平位置、像素语义分类、像素几何分类以及图像区域），用来评估场景语义理解方法。</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/pic20.png" alt="pic20" /></p>

<p>SiftFlow [41]：包含2688张完全标注的图像，是LabelMe数据集[59]的子集。多数图像基于8种不同的户外场景，包括街道、高山、田地、沙滩、建筑等。图像是256x256的，分别属于33个语义类别。未标注的或者标为其他语义类别的像素被认为是空。</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/pic21.png" alt="pic21" /></p>

<h2 id="32--25维数据集">3.2  2.5维数据集</h2>

<p>随着廉价的扫描器的到来，带有深度信息的数据集开始出现并被广泛使用。本章，我们回顾最知名的2.5维数据集，其中包含了深度信息。</p>

<p><a href="http://cs.nyu.edu/∼silberman/projects/indoor scene seg sup.html">NYUDv2数据集</a>包含1449张由微软Kinect设备捕获的室内的RGB-D图像。其给出密集的像素级别的标注（类别级别和实力级别的均有），训练集795张与测试集654张均有40个室内物体的类[60]，该数据集由于其刻画室内场景而格外重要，使得它可以用于某种家庭机器人的训练任务。但是，它相对于其他数据集规模较小，限制了其在深度网络中的应用。</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/pic22.png" alt="pic22" /></p>

<p><a href="http://sun3d.cs.princeton.edu/">SUN3D数据集</a>：与NYUDv2数据集相似，该数据集包含了一个大规模的RGB-D视频数据集，包含8个标注了的序列。每一帧均包含场景中物体的语义分割信息以及摄像机位态信息。该数据集还在扩充中，将会包含415个序列，在41座建筑中的254个空间中获取。另外，某些地方将会在一天中的多个时段被重复拍摄。</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/pic23.png" alt="pic23" /></p>

<p><a href="http://rgbd.cs.princeton.edu/">SUNRGBD数据集</a>由四个RGB-D传感器得来，包含10000张RGB-D图像，尺寸与PASCAL VOC一致。该数据集包含了NYU depth v2 [46], Berkeley B3DO [61], 以及SUN3D [47]数据集中的图像，整个数据集均为密集标注，包括多边形、带方向的边界框以及三维空间，适合于场景理解任务。</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/pic24.png" alt="pic24" /></p>

<p><a href="http://www.acin.tuwien.ac.at/?id=289">物体分割数据集(OSD)</a>该数据集用来处理未知物体的分割问题，甚至是在部分遮挡的情况下进行处理。该数据集有111个实例，提供了深度信息与颜色信息，每张图均进行了像素级别的标注，以此来评估物体分割方法。但是，该数据集并没有区分各个类，使其退化为一个二值化的数据集，包含物体与非物体两个类。</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/pic25.png" alt="pic25" /></p>

<p><a href="http://rgbd-dataset.cs.washington.edu/">RGB-D物体数据集</a>该数据集由视频序列构成，有300个常见的室内物体，分为51个类，使用WordNet hypernym-hyponym关系进行分类。该数据集使用Kinect型三维摄像机进行摄制，640x480RGB图像，深度信息30赫兹。对每一帧，数据集提供了RGB-D及深度信息，这其中包含了物体、位置及像素级别的标注。另外，每个物体放在旋转的桌面上以得出360度的视频序列。对于验证过程，其提供了22个标注的自然室内场景的包含物体的视频序列。</p>

<p><img src="/assets/images/posts/2019-07-06-semantic-segmentaion/pic26.png" alt="pic26" /></p>

:ET