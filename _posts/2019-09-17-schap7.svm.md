---
title:  支持向量机
date: 2019-09-17 17:04:13 +0800
description:
image:
  path: /assets/images/posts/2019-09-17-schap7.svm/cover.jpg
  thumbnail: /assets/images/posts/2019-09-17-schap7.svm/thumb.jpg
categories:
  - ai
tags:
  - 统计学习方法
  - 机器学习实战
  - 编程
---

支持向量机是一种分类模型，它可以在线性可分的数据中找到一个超平面，把数据分成两类。在超平面一侧的数据为正类，另一侧的为负类。

那么，我们拿到数据集 $D = \{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\}$，怎么找超平面呢？

首先，我们定义超平面的方程为 $w^T \cdot x + b = 0$，简写成 $(w, b)$ 。那么，数据集中的样本 $(x_i, y_i)$ 到这个平面的距离可以表示成： $\dfrac{y_i(w^T \cdot x_i + b)}{\mid\mid w \mid\mid}$ 。支持向量机的思想是让支持向量到超平面的距离尽可能大，而支持向量就是到超平面距离最小的样本点，即 $\mathop{\arg\min}\limits_{i} \dfrac{y_i(w^T \cdot x_i + b)}{\mid\mid w \mid\mid}$ 得到的各个样本 $(x_i, y_i)$。然后再选取超平面 $(w, b)$ 让这个距离最大，即：

$$
\mathop{\arg\max}\limits_{(w, b)}\left[\min\limits_{i} \dfrac{y_i(w^T \cdot x_i + b)}{\mid\mid w \mid\mid} \right]
$$ 

求出上面的 $(w, b)$ 就是我们需要的超平面。

下面就是求解的过程。

我们知道， $y_i(w^T \cdot x_i + b) \ge 1$ 而且只有支持向量才能取到 1，所以我们把上式写成：

$$
\mathop{\arg\max}\limits_{(w, b)}\left[\min\limits_{i} \dfrac{1}{\mid\mid w \mid\mid} \right] = \mathop{\arg\max}\limits_{(w, b)} \dfrac{1}{\mid\mid w \mid\mid} = \mathop{\arg\min}\limits_{(w, b)}\mid\mid w \mid\mid = \dfrac{1}{2}\mathop{\arg\min}\limits_{(wb)} \mid\mid w \mid\mid^2
$$


这就是 SVM 学习的最优化问题。整理一下就是：

对于一个训练数据集 $D = \{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\}$，要得到最大分隔超平面和分类决策函数。

就是要求最优化问题：

$$\mathop{\arg\min}\limits_{(w, b)} \dfrac{1}{2} \mid\mid w \mid\mid ^2$$

约束条件：

$$y_i(w^T \cdot x_i + b) - 1 \ge 0$$

求出后得到最大分隔超平面：

$$w^* \cdot x + b^* = 0$$

分类决策函数：

$$f(x) = sign(w^* \cdot x + b^*)$$

下面我们来看一个实际例子：

对于一个训练数据集，其正样本点是 $x_1 = (3, 3)^T, x_2 = (4, 3)^{T}$ ，负样本点是 $x_3 = (1, 1)^{T}$ ，如何找到最大间隔分离超平面？

因为最大分隔距离跟 $\dfrac{1}{\mid\mid w \mid\mid}$ 有关，所以倒过来最大变成最小，得到 $\mathop{\arg\min}\limits_{(w, b)} \dfrac{1}{2} \mid\mid w \mid\mid ^2$

对于本题：

$$\mathop{\arg\min}\limits_{(w, b)} \dfrac{1}{2} \mid\mid w \mid\mid ^2 = \mathop{\arg\min}\limits_{(w, b)} \dfrac{1}{2} (w_1^2 + w_2^2)$$

约束条件为：$y_i(w^T \cdot x_i + b) \ge 1$，所以：

$$1 \cdot (3w_1 + 3w_2 + b) \ge 1$$

$$1 \cdot (4w_1 + 3w_2 + b) \ge 1$$

$$-1 \cdot (w_1 + w_2 + b) \ge 1$$

在约束条件下就出最优化问题的解：$w_1 = w_2 = \dfrac{1}{2}, b = -2$ ，所以分隔超平面为：

$$\dfrac{1}{2} x^{(1)} + \dfrac{1}{2} x^{(2)} - 2 = 0$$

其中，$x_1 = (3, 3)^T, x_3 = (1, 1)^T$ 为支持向量

## 对偶算法

SVM 最优化问题：

$$\mathop{\arg\min}\limits_{(w, b)} \dfrac{1}{2} \mid\mid w \mid\mid ^2$$

约束条件：

$$y_i(w^T \cdot x_i + b) - 1 \ge 0$$

这个 SVM 最优化问题的求解，要先确定约束条件的范围，然后在范围中找最值，这个过程比较复杂。我们希望通过一个式子把最值和约束条件都包含进来，这就是拉格朗日函数法。我们引入拉格朗日乘子 $\alpha_i \ge 0, i = 1, 2, ..., N$ 构造拉格朗日函数：

$$L(w, b, \alpha) = \dfrac{1}{2} \mid\mid w \mid\mid ^2 - \sum\limits_{i=1}^{N} \alpha_i \left[ y_i(w^T \cdot x_i + b) - 1\right]$$

再有拉格朗日对偶性原理，原始问题的对偶问题是极大极小问题：

$$\mathop{\arg\max}\limits_{\alpha} \left[ \mathop{\arg\min}\limits_{(w, b)} L(w, b, \alpha)\right]$$

对于 $\mathop{\arg\min}\limits_{(w, b)}$ 的部分，只要求 $\dfrac{\partial L}{\partial w}$ 和 $\dfrac{\partial L}{\partial b}$ 就可以了，于是有：

$$\dfrac{\partial L}{\partial w} = w - \sum\limits_{i=1}^{N} \alpha_i y_i x_i = 0$$

$$\dfrac{\partial L}{\partial b} = \sum\limits_{i=1}^{N} \alpha_i y_i = 0$$


这样求出 $w^* = \sum\limits_{i=1}^{N} \alpha_i y_i x_i$， 在带入 $L(w, b, \alpha)$ 就可以得到：

$$\begin{aligned} & \mathop{\arg\max}\limits_{\alpha} \left\{ \dfrac{1}{2} \left(\sum\limits_{i=1}^{N} \alpha_iy_ix_i \right)^2 - \sum\limits_{i=1}^{N} \alpha_i \left[ y_i\left((\sum\limits_{j=1}^{N} \alpha_jy_jx_j) \cdot x_i + b\right) - 1 \right] \right\} \\ &= \mathop{\arg\max}\limits_{\alpha}\dfrac{1}{2} \sum\limits_{i=1}^{N} \sum\limits_{j=1}^{N} \alpha_i \alpha_j y_iy_j x_ix_j - \sum\limits_{i=1}^{N} \sum\limits_{j=1}^{N} \alpha_i \alpha_j y_iy_j x_ix_j + b \cdot \sum\limits_{i=1}^{N} \alpha_iy_i + \sum\limits_{i=1}^{N} \alpha_i \end{aligned}$$

又因为 $b \cdot \sum\limits_{i=1}^{N} \alpha_iy_i = b\cdot 0 = 0$ ，所以：



$$\mathop{\arg\min}\limits_{(w, b)} L(w, b, \alpha) = - \dfrac{1}{2} \sum\limits_{i=1}^{N} \sum\limits_{j=1}^{N} \alpha_i \alpha_j y_iy_j (x_i \cdot x_j) + \sum\limits_{i=1}^{N} \alpha_i$$

那么原始问题的对偶问题就是：

$$\mathop{\arg\max}\limits_{\alpha} \left[ \mathop{\arg\min}\limits_{(w, b)} L(w, b, \alpha)\right] = \mathop{\arg\max}\limits_{\alpha} \left\{ - \dfrac{1}{2} \sum\limits_{i=1}^{N} \sum\limits_{j=1}^{N} \alpha_i \alpha_j y_iy_j (x_i \cdot x_j) + \sum\limits_{i=1}^{N} \alpha_i \right\}$$

加上负号，最大变最小：

$$ \mathop{\arg\min}\limits_{\alpha} \left\{ \dfrac{1}{2} \sum\limits_{i=1}^{N} \sum\limits_{j=1}^{N} \alpha_i \alpha_j y_iy_j (x_i \cdot x_j) - \sum\limits_{i=1}^{N} \alpha_i \right\}$$

约束条件：

$$\sum\limits_{i=1}^{N} \alpha_iy_i = 0$$

$$\alpha_i \ge 0$$

上面就是对偶最优化问题。当我们求出最优解 $\alpha^{*}$ 后，可以找到一个使 $\alpha_{j}^{*} > 0$ 的 $j$，然后求出对应的 $(w^{*}, b^{*})$：

 $$w^* = \sum\limits_{i=1}^{N} \alpha_i^* y_i x_i$$

其中至少有一个 $\alpha_i^* > 0$ ，对此 $j$ 有：

$$y_j(w^* \cdot x_j + b^*) - 1 = 0$$

把 $w^*$ 代入，并且有 $y_j^2 = 1$ ，则：

$$b^* = y_j - \sum\limits_{i=1}^{N} \alpha_i^* y_i (x_i \cdot x_j)$$

## 总结

对于给定的训练数据集，可以先求出对偶问题的 $\alpha^{*}$，再求 $(w^{*}, b^{*})$，从而得到分隔超平面以及分类决策函数。这种算法称为**支持向量机的对偶学习算法**




















