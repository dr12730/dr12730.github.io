---
title:  支持向量机
date: 2019-09-17 17:04:13 +0800
description:
image:
  path: /assets/images/posts/2019-09-17-schap7.svm/cover.jpg
  thumbnail: /assets/images/posts/2019-09-17-schap7.svm/thumb.jpg
categories:
  - ai
tags:
  - 统计学习方法
  - 机器学习实战
  - 编程
---


<!-- vim-markdown-toc GFM -->

* [1 硬间隔最大化的线性支持向量机](#1-硬间隔最大化的线性支持向量机)
    * [1.1 对偶算法](#11-对偶算法)
    * [1.2 总结](#12-总结)
* [2 软间隔最大化的线性支持向量机](#2-软间隔最大化的线性支持向量机)
    * [2.1 学习算法的对偶算法](#21-学习算法的对偶算法)
    * [2.2 总结](#22-总结)
* [3 序列最小最优化算法 SMO](#3-序列最小最优化算法-smo)
    * [3.1 求解两个变量二次规划问题的解析方法](#31-求解两个变量二次规划问题的解析方法)
    * [3.2 变量的选择](#32-变量的选择)
* [4 计算阈值 b 和差值 $E_1$](#4-计算阈值-b-和差值-e_1)
    * [4.1 更新 b 值](#41-更新-b-值)
    * [4.2 更新 $E_1$](#42-更新-e_1)

<!-- vim-markdown-toc -->

## 1 硬间隔最大化的线性支持向量机

支持向量机是一种分类模型，它可以在**线性可分的数据**中找到一个超平面，把数据分成两类。在超平面一侧的数据为正类，另一侧的为负类。

那么，我们拿到数据集 $D = \{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\}$，怎么找超平面呢？

首先，我们定义超平面的方程为 $w^T \cdot x + b = 0$，简写成 $(w, b)$ 。那么，数据集中的样本 $(x_i, y_i)$ 到这个平面的距离可以表示成： $\dfrac{y_i(w^T \cdot x_i + b)}{\parallel w \parallel}$ 。支持向量机的思想是让支持向量到超平面的距离尽可能大，而支持向量就是到超平面距离最小的样本点，即 $\mathop{\arg\min}\limits_{i} \dfrac{y_i(w^T \cdot x_i + b)}{\parallel w \parallel}$ 得到的各个样本 $(x_i, y_i)$。然后再选取超平面 $(w, b)$ 让这个距离最大，即：

$$
\mathop{\arg\max}\limits_{(w, b)}\left[\min\limits_{i} \dfrac{y_i(w^T \cdot x_i + b)}{\parallel w \parallel} \right]
$$ 

求出上面的 $(w, b)$ 就是我们需要的超平面。

下面就是求解的过程。

我们知道， $y_i(w^T \cdot x_i + b) \ge 1$ 而且只有支持向量才能取到 1，所以我们把上式写成：

$$
\mathop{\arg\max}\limits_{(w, b)}\left[\min\limits_{i} \dfrac{1}{\parallel w \parallel} \right] = \mathop{\arg\max}\limits_{(w, b)} \dfrac{1}{\parallel w \parallel} = \mathop{\arg\min}\limits_{(w, b)}\parallel w \parallel = \dfrac{1}{2}\mathop{\arg\min}\limits_{(wb)} \parallel w \parallel^2
$$


这就是 SVM 学习的最优化问题。整理一下就是：

对于一个训练数据集 $D = \{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\}$，要得到最大分隔超平面和分类决策函数。

就是要求最优化问题：

$$\mathop{\arg\min}\limits_{(w, b)} \dfrac{1}{2} \parallel w \parallel^2 \tag{1.1}$$

约束条件：

$$y_i(w^T \cdot x_i + b) - 1 \ge 0 \tag{1.2}$$

求出后得到最大分隔超平面：

$$w^\ast \cdot x + b^\ast = 0 \tag{1.3}$$

分类决策函数：

$$f(x) = sign(w^\ast \cdot x + b^\ast) \tag{1.4}$$

下面我们来看一个实际例子：

对于一个训练数据集，其正样本点是 $x_1 = (3, 3)^T, x_2 = (4, 3)^{T}$ ，负样本点是 $x_3 = (1, 1)^{T}$ ，如何找到最大间隔分离超平面？

因为最大分隔距离跟 $\dfrac{1}{\parallel w \parallel}$ 有关，所以倒过来最大变成最小，得到 $\mathop{\arg\min}\limits_{(w, b)} \dfrac{1}{2} \parallel w \parallel ^2$

对于本题：

$$\mathop{\arg\min}\limits_{(w, b)} \dfrac{1}{2} \parallel w \parallel^2 = \mathop{\arg\min}\limits_{(w, b)} \dfrac{1}{2} (w_1^2 + w_2^2)$$

约束条件为：$y_i(w^T \cdot x_i + b) \ge 1$，所以：

$$1 \cdot (3w_1 + 3w_2 + b) \ge 1$$

$$1 \cdot (4w_1 + 3w_2 + b) \ge 1$$

$$-1 \cdot (w_1 + w_2 + b) \ge 1$$

在约束条件下就出最优化问题的解：$w_1 = w_2 = \dfrac{1}{2}, b = -2$ ，所以分隔超平面为：

$$\dfrac{1}{2} x^{(1)} + \dfrac{1}{2} x^{(2)} - 2 = 0$$

其中，$x_1 = (3, 3)^T, x_3 = (1, 1)^T$ 为支持向量

### 1.1 对偶算法

SVM 最优化问题：

$$\mathop{\arg\min}\limits_{(w, b)} \dfrac{1}{2} \parallel w \parallel^2 \tag{1.5}$$

约束条件：

$$y_i(w^T \cdot x_i + b) - 1 \ge 0 \tag{1.6}$$

这个 SVM 最优化问题的求解，要先确定约束条件的范围，然后在范围中找最值，这个过程比较复杂。我们希望通过一个式子把最值和约束条件都包含进来，这就是拉格朗日函数法。我们引入拉格朗日乘子 $\alpha_i \ge 0, i = 1, 2, ..., N$ 构造拉格朗日函数：

$$L(w, b, \alpha) = \dfrac{1}{2} \parallel w \parallel^2 - \sum\limits_{i=1}^{N} \alpha_i \left[ y_i(w^T \cdot x_i + b) - 1\right]$$

再有拉格朗日对偶性原理，原始问题的对偶问题是极大极小问题：

$$\mathop{\arg\max}\limits_{\alpha} \left[ \mathop{\arg\min}\limits_{(w, b)} L(w, b, \alpha)\right]$$

对于 $\mathop{\arg\min}\limits_{(w, b)}$ 的部分，只要求 $\dfrac{\partial L}{\partial w}$ 和 $\dfrac{\partial L}{\partial b}$ 就可以了，于是有：

$$\dfrac{\partial L}{\partial w} = w - \sum\limits_{i=1}^{N} \alpha_i y_i x_i = 0$$

$$\dfrac{\partial L}{\partial b} = \sum\limits_{i=1}^{N} \alpha_i y_i = 0$$


这样求出 $w^* = \sum\limits_{i=1}^{N} \alpha_i y_i x_i$， 在带入 $L(w, b, \alpha)$ 就可以得到：

$$\begin{aligned} & \mathop{\arg\max}\limits_{\alpha} \left\{ \dfrac{1}{2} \left(\sum\limits_{i=1}^{N} \alpha_iy_ix_i \right)^2 - \sum\limits_{i=1}^{N} \alpha_i \left[ y_i\left((\sum\limits_{j=1}^{N} \alpha_jy_jx_j) \cdot x_i + b\right) - 1 \right] \right\} \\ &= \mathop{\arg\max}\limits_{\alpha}\dfrac{1}{2} \sum\limits_{i=1}^{N} \sum\limits_{j=1}^{N} \alpha_i \alpha_j y_iy_j x_ix_j - \sum\limits_{i=1}^{N} \sum\limits_{j=1}^{N} \alpha_i \alpha_j y_iy_j x_ix_j + b \cdot \sum\limits_{i=1}^{N} \alpha_iy_i + \sum\limits_{i=1}^{N} \alpha_i \end{aligned}$$

又因为 $b \cdot \sum\limits_{i=1}^{N} \alpha_iy_i = b\cdot 0 = 0$ ，所以：



$$\mathop{\arg\min}\limits_{(w, b)} L(w, b, \alpha) = - \dfrac{1}{2} \sum\limits_{i=1}^{N} \sum\limits_{j=1}^{N} \alpha_i \alpha_j y_iy_j (x_i \cdot x_j) + \sum\limits_{i=1}^{N} \alpha_i$$

那么原始问题的对偶问题就是：

$$\mathop{\arg\max}\limits_{\alpha} \left[ \mathop{\arg\min}\limits_{(w, b)} L(w, b, \alpha)\right] = \mathop{\arg\max}\limits_{\alpha} \left\{ - \dfrac{1}{2} \sum\limits_{i=1}^{N} \sum\limits_{j=1}^{N} \alpha_i \alpha_j y_iy_j (x_i \cdot x_j) + \sum\limits_{i=1}^{N} \alpha_i \right\}$$

加上负号，最大变最小：

$$ \mathop{\arg\min}\limits_{\alpha} \left\{ \dfrac{1}{2} \sum\limits_{i=1}^{N} \sum\limits_{j=1}^{N} \alpha_i \alpha_j y_iy_j (x_i \cdot x_j) - \sum\limits_{i=1}^{N} \alpha_i \right\} \tag{1.7}$$

约束条件：

$$\sum\limits_{i=1}^{N} \alpha_iy_i = 0 \tag{1.8}$$

$$\alpha_i \ge 0 \tag{1.9}$$

上面就是对偶最优化问题。当我们求出最优解 $\alpha^{\ast} $ 后，可以找到一个使 $\alpha_{j}^{\ast} > 0$ 的 $j$，然后求出对应的 $(w^{\ast}, b^{\ast})$：

 $$w^* = \sum\limits_{i=1}^{N} \alpha_i^* y_i x_i \tag{1.10}$$

其中至少有一个 $\alpha_i^* > 0$ ，对此 $j$ 有：

$$y_j(w^* \cdot x_j + b^*) - 1 = 0$$

把 $w^*$ 代入，并且有 $y_j^2 = 1$ ，则：

$$b^* = y_j - \sum\limits_{i=1}^{N} \alpha_i^* y_i (x_i \cdot x_j) \tag{1.11}$$

### 1.2 总结

对于给定的训练数据集，可以先求出对偶问题的 $\alpha^{\ast}$，再求 $(w^{\ast}, b^{\ast})$，从而得到分隔超平面以及分类决策函数。这种算法称为**支持向量机的对偶学习算法**

## 2 软间隔最大化的线性支持向量机

上面的支持向量机学习算法只针对于**线性可分数据**，对于线性不可分的数据，因为上面的约束条件不成立，所以不适用。

> 这里的线性不可分数据应该解释为：数据中存在少许特异样本点，除去这些外的数据集是线性可分的。

线性不可分意味着对于特异样本点 $(x_i, y_i)$ 它不满足 $y_i(w^T \cdot x_i + b) \ge 1$。我们可以让它加上一个松弛变量 $\xi_i$，使 $y_i(w^T \cdot x_i + b) + \xi_i \ge 1$，这就是软间隔。多增加的松弛变量，也需要给出代价，所以优化目标变为 $\dfrac{1}{2} \parallel w \parallel^2 + C \sum\limits_{i=1}^{N} \xi_i$ 。这里我们给每一个样本都加上了一个松弛变量。

这样，软间隔最大化的线性支持向量机学习算法转化为凸优化二次问题（原始问题）

$$\mathop{\arg\min}\limits_{w, b, \xi} \left[\dfrac{1}{2} \parallel w \parallel^2 + C \sum\limits_{i=1}^{N} \xi_i \right] \tag{2.1}\label{2.1}$$

s.t：

$$ y_i(w^T \cdot x_i + b) + \xi_i \ge 1 \tag{2.2}\label{2.2}$$

$$\xi_i \ge 0 \tag{2.3}\label{2.3}$$

> 最小化目标函数 $\eqref{2.1}$ 包含两层含义：使 $\dfrac{1}{2} \parallel w \parallel ^2$ 尽可能小即间隔尽可能大，同时让落在分隔区的特异样本尽可能少（误分类点尽可能少），C 调和两者的系数

通过学习算法即软间隔最大化，可以找到 $(w^{\ast}, b^{\ast})$ ，从而得到分离超平面和分类决策函数。分离超平面和相应的分类决策函数一起，就成为线性支持向量机

### 2.1 学习算法的对偶算法

原始问题  $\eqref{2.1}, \eqref{2.2}, \eqref{2.3}$ 的对偶问题如下：

$$ \mathop{\arg\min}\limits_{\alpha} \left\{ \dfrac{1}{2} \sum\limits_{i=1}^{N} \sum\limits_{j=1}^{N} \alpha_i \alpha_j y_iy_j (x_i \cdot x_j) - \sum\limits_{i=1}^{N} \alpha_i \right\} \tag{2.3}$$

s.t.

$$\sum\limits_{i=1}^{N} a_iy_i =0 \tag{2.4}$$

$$0 \le \alpha_i \le C \tag{2.5}$$

通过求解上式可以得到解 $\alpha^{\ast} = (\alpha_1^{\ast}, \alpha_2^{\ast}, \dots, \alpha_N^{\ast})^T$，取一个分量 $0 \le \alpha_j^{\ast} \le C$ ，求出 $(w^{\ast}, b^{\ast})$ ：

$$w^{*} = \sum\limits_{i=1}^{N} \alpha_i^{*} y_i x_i \tag{2.6}\label{2.6}$$

$$b^{*} = y_j - \sum\limits_{i=1}^{N} y_i \alpha_i^{*} (x_i \cdot x_j) \tag{2.7}\label{2.7}$$

### 2.2 总结

线性支持向量机学习算法：

**输入**：训练数据集 $D = \{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\}$

**输出**：分离超平面和分类决策函数

1. 选择惩罚参数 C > 0，构造并求解凸二次优化问题： $\eqref{2.3}, \eqref{2.4}, \eqref{2.5}$

    求得最优解 $\alpha^{\ast} = (\alpha_1^{\ast}, \alpha_2^{\ast}, \dots, \alpha_N^{\ast})^T$

2. 计算 $\eqref{2.6}$

    选择 $\alpha^{\ast}$ 的一个分量 $\alpha^{\ast}_j$ 计算 $\eqref{2.7}$ 

3. 求得分离超平面 $w^{\ast} \cdot x + b^{\ast} = 0$ 

    分类决策函数： $f(x) = sign(w^{\ast} \cdot x + b^{\ast})$


## 3 序列最小最优化算法 SMO

SMO 算法要解如下凸二次规划的对偶问题：

$$ \mathop{\arg\min}\limits_{\alpha} \left\{ \dfrac{1}{2} \sum\limits_{i=1}^{N} \sum\limits_{j=1}^{N} \alpha_i \alpha_j y_iy_j K(x_i, x_j) - \sum\limits_{i=1}^{N} \alpha_i \right\} \tag{2.3}$$

s.t.

$$\sum\limits_{i=1}^{N} a_iy_i =0 \tag{2.4}$$

$$0 \le \alpha_i \le C \tag{2.5}$$

这个问题中的变量是拉格朗日乘子，一个变量 $\alpha_i$ 对应一个样本点

SMO 的思想是：如果所有变量的解都满足 KKT 条件，那么最优解就找到了；否则，每次取出两个变量，固定其他变量，求这两个变量的二次规划问题的解。

SMO 算法包括两个部分：一个是求解二次规划问题解析方法，另一个是选择两个变量的启发式方法。

### 3.1 求解两个变量二次规划问题的解析方法

最优化问题的未剪辑解为：

$$\alpha_2^{new, unc} = \alpha_2^{old} + \dfrac{y_2(E_1 - E_2)}{\eta} \tag{3.1}$$

其中： 

$$\eta = K_{11} + K_{22} - 2 K_{12}$$

经过剪辑后的解 $\alpha_2^{new} = clip(\alpha_2^{new, unc}, L, H)$

$$\alpha_1^{new} = \alpha_1^{old} + y_1y_2 (\alpha_2^{old} - \alpha_2^{new}) \tag{3.2}$$

> 其中：
> - $y_1 = y_2$
> $$L = \max(0, \alpha_2^{old} - \alpha_1^{old}), H = \min(C, C + \alpha_2^{odl} = \alpha_1^{old})$$
> 
> - $y_1 \neq y_2$
> $$L = \max(0, \alpha_2^{old} + \alpha_1^{old}-C), H = \min(C, \alpha_2^{old} - \alpha_1^{old})$$


### 3.2 变量的选择

1. 第 1 个变量的选择

    遍历所有样本，选择最违背 KKT 条件的样本，选择它对应的参数 $\alpha_i$ 为第一个变量。样本的 KKT 条件与参数的关系：

    $$\begin{aligned}  \alpha_i = 0 &\iff& y_i g(x_i) \ge 1 \\ 0 < \alpha_i < C &\iff& y_i g(x_i) = 1 \\ \alpha_i = C &\iff& y_i g(x_i) \le 1 \end{aligned}$$

    先检查间隔边界上的样本点，如果它们满足在检查其他样本点

2. 第 2 个变量的选择

    第 2 个变量的选择希望能使 $\alpha_2$ 有足够大的变化。又因为 $\alpha_2^{new}$ 依赖于 $\| E_1 - E_2 \|$，所以选择使其最大的 $\alpha_2$

## 4 计算阈值 b 和差值 $E_1$

### 4.1 更新 b 值
完成两个变量的优化之后，要重新计算 b。

当 $0 < \alpha_1^{new} < C$ 时：

$$b_1^{new} = -E_1 - y_1 K_{11} (\alpha_1^{new} - \alpha_1^{old}) - y_2 K_{21} (\alpha_2^{new} - \alpha_2^{old}) + b^{old} \tag{4.1}$$

同样的，如果 $0 < \alpha_2^{new} < C$，那么：


$$b_2^{new} = -E_2 - y_1 K_{12} (\alpha_1^{new} - \alpha_1^{old}) - y_2 K_{22} (\alpha_2^{new} - \alpha_2^{old}) + b^{old} \tag{4.2}$$

1. 如果 $\alpha_1, \alpha_2$ 同时满足条件[0, C]，那么 $b_1^{new} = b_2^{new}$
2. 如果 $\alpha_1^{new}, \alpha_2^{new}$ 是 0 或 c，那么选择中点作为 $b^{new}$
 
### 4.2 更新 $E_1$

$$E_i^{new} = \sum\limits_{s} y_j \alpha_j K(x_i, x_j) + b^{new} - y_i$$

其中 $S$ 是所有支持向量 $x_j$ 的集合。

