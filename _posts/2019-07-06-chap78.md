---
title: Linux 之RAID、LVM
date: 2019-07-06 11:59:09 +0800
description:
image:
  path: /assets/images/posts/2019-07-06-chap78/cover.jpg
  thumbnail: /assets/images/posts/2019-07-06-chap78/thumb.jpg
categories:
  - it
tags:
  - linux
---

RAID - Redundant Array of Independent Disks，独立冗余磁盘阵列，是为了满足生产环境对硬盘的 IO 读写速度和备份机制的需求而产生的技术。再考虑到动态调整存储资源，还将介绍 LVM(Logical Volume Manager，逻辑卷管理器)的部署。

# 磁盘阵列 RAID

RAID 通过把多个硬盘设备组合成一个容量更大、速度更快、安全性更好的磁盘阵列，并把数据切割成多个区段后放在不同的物理硬盘上，通过分散读写技术提高阵列性能；又把重要数据的副本同步到不同物理硬盘上，实现了数据备份的功能。

目前已有的 RAID 技术有十几种之多，本章记录 RAID0、RAID1、RAID5 和 RAID10 方案。

## RAID 类别

### RAID 0

![raid0](/assets/images/posts/2019-07-06-chap78/raid0.png)

把多块硬盘设备串联成一个卷组，把数据依次写入各物理硬盘中，实现读写速度数倍提升，但一块硬盘故障将导致整个数据被破坏。

### RAID 1

![raid1](/assets/images/posts/2019-07-06-chap78/raid1.jpg)

把两块以上硬盘并联，将数据同时写入多块硬盘，当某块硬盘发生故障时，立即自动热交换恢复数据。

虽然安全性提高，但可用率降低，两块硬盘组成的 RAID1 可用率为 50%，三块为 33%

### RAID 5

![raid5](/assets/images/posts/2019-07-06-chap78/raid5.png)

RAID5 是把硬盘数据奇偶校验信息保存到了其他硬盘，当任何一个硬盘故障时，通过奇偶校验信息来尝试重建数据

### RAID 10

![raid10](/assets/images/posts/2019-07-06-chap78/raid10.png)

RAID 10 是 RAID0 + RAID1 的组合。它至少需要 4 块硬盘组建。其中两两组建成 RAID1 阵列，以保证数据安全；再将两个 RIAD1 阵列组成 RAID0 阵列，以提高读写速度。RAID10 在不考虑成本的情况下，性能超过了 RAID5，当前已成为广泛使用的存储技术

## 部署磁盘阵列

我们可以虚拟几块硬盘设备，通过 udev 的监控，会在 `/dev/` 目录有所有体现，现假定有 4 块硬盘设备，分别为：`/dev/sda, /dev/sdb, /dev/sdc, /dev/sde`，后面我们将用它们来部署 RAID 磁盘阵列

### 阵列管理器 mdadm

用于管理 Linux 系统的 RAID 磁盘阵列的程序是 mdadm，其命令格式如下：

`mdadm [模式] <RAID设备名称> [选项] [成员设备名称]`

参数：

|------|------------------|
| 参数 | 作用 |
|------|------------------|
| -a | 检测设备名称 |
| -n | 指定设备数量 |
| -l | 指定 RAID 级别 |
| -C | 创建 |
| -v | 显示过程 |
| -f | 模拟设备损坏 |
| -r | 移除设备 |
| -Q | 查看摘要信息 |
| -D | 查看详细信息 |
| -S | 停止 RAID 磁盘阵列 |
|------|------------------|

#### 创建 RAID 10 磁盘阵列

我们用 `mdadm` 命令创建 RAID 10 阵列，名称为 `/dev/md0`

1. 创建磁盘阵列设备

   ```bash
   >>> mdadm -Cv /dev/md0 -a yes -n 4 -l 10 /dev/sdb /dev/sdc /dev/sdd /dev/sde
   mdadm: layout defaults to n2
   mdadm: layout defaults to n2
   mdadm: chunk size defaults to 512K
   mdadm: size set to 20954624K
   mdadm: Defaulting to version 1.2 metadata
   mdadm: array /dev/md0 started.
   ```

   其中：

   - `-C`：代表创建一个 RAID 阵列卡
   - `-v`：显示创建的过程
   - `/dev/md0`：磁盘阵列名称
   - `-a yes`：自动创建设备文件
   - `-n 4`：使用 4 块硬盘组成阵列
   - `-l 10`：选择 RAID 10 方案

2. 将阵列格式化为 ext4 格式

   ```bash
   >>> mkfs.ext4 /dev/md0
   mke2fs 1.42.9 (28-Dec-2013)
   Filesystem label=
   OS type: Linux
   Block size=4096 (log=2)
   Fragment size=4096 (log=2)
   Stride=128 blocks, Stripe width=256 blocks
   2621440 inodes, 10477312 blocks
   ```

3. 挂载格式化好的存储设备

   ```bash
   >>> mkdir /RAID
   >>> mount /dev/md0 /RAID
   >>> df -h
   Filesystem Size Used Avail Use% Mounted on
   /dev/mapper/rhel-root 18G 3.0G 15G 17% /
   devtmpfs 905M 0 905M 0% /dev
   /dev/md0 40G 49M 38G 1% /RAID
   ```

4. 查看阵列详细信息，并实现自动挂载

   ```bash
   >>> mdadm -D /dev/md0
   /dev/md0:
   Version : 1.2
   Creation Time : Tue May 5 07:43:26 2017
   Raid Level : raid10
   Array Size : 41909248 (39.97 GiB 42.92 GB)
   ...
   Number Major Minor RaidDevice State
   0 8 16 0 active sync /dev/sdb
   1 8 32 1 active sync /dev/sdc
   2 8 48 2 active sync /dev/sdd
   3 8 64 3 active sync /dev/sde

   >>> echo "/dev/md0 /RAID ext4 defaults 0 0" >> /etc/fstab
   ```

### 磁盘阵列损坏及修复

当硬盘损坏之后，使用 `mdadm` 移除设备

```bash
# 报告设备损坏
>>> mdadm /dev/md0 -f /dev/sdb
mdadm: set /dev/sdb faulty in /dev/md0

>>> mdadm -D /dev/md0
/dev/md0:
Version : 1.2
Creation Time : Fri May 8 08:11:00 2017
Raid Level : raid10
Array Size : 41909248 (39.97 GiB 42.92 GB)
...
Active Devices : 3
Working Devices : 3
Failed Devices : 1
Spare Devices : 0
...
1 8 32 1 active sync /dev/sdc
2 8 48 2 active sync /dev/sdd
3 8 64 3 active sync /dev/sde
0 8 16 - faulty /dev/sdb
```

硬盘损坏不会影响数据，现在更换新的硬盘后，再通过 `mdadm` 替换

```bash
>>> umount /RAID
# 添加新的硬盘，udev 监控并命名为 /dev/sdb
>>> mdadm /dev/md0 -a /dev/sdb
>>> mdadm -D /dev/md0
/dev/md0:
Version : 1.2
Creation Time : Mon Jan 30 00:08:56 2017
Raid Level : raid10
Array Size : 41909248 (39.97 GiB 42.92 GB)
Used Dev Size : 20954624 (19.98 GiB 21.46 GB)
...
Number Major Minor RaidDevice State
4 8 16 0 active sync /dev/sdb
1 8 32 1 active sync /dev/sdc
2 8 48 2 active sync /dev/sdd
3 8 64 3 active sync /dev/sde

>>> mount -a
```

### 磁盘阵列 + 备份盘

为了防止 RAID 10 中的 RAID1 同时两块硬盘故障的情况，我们可以再加入一块热备份盘，这块硬盘平时处于闲置状态，当磁盘阵列中有硬盘故障后，它会自动替换。这里通过 RAID 5 来演示

```bash
>>> mdadm -Cv /dev/md0 -n 3 -l 5 -x 1 /dev/sdb /dev/sdc /dev/sde
mdadm: layout defaults to left-symmetric
mdadm: layout defaults to left-symmetric
mdadm: chunk size defaults to 51
...

>>> mdadm -D /dev/md0
/dev/md0:
Version : 1.2
Creation Time : Fri May 8 09:20:35 2017
Raid Level : raid5
Array Size : 41909248 (39.97 GiB 42.92 GB)
Used Dev Size : 20954624 (19.98 GiB 21.46 GB)
Raid Devices : 3
Total Devices : 4
...
Number Major Minor RaidDevice State
0 8 16 0 active sync /dev/sdb
1 8 32 1 active sync /dev/sdc
4 8 48 2 active sync /dev/sdd
3 8 64 - spare /dev/sde

# 将阵列格式化为 ext4
>>> mkfs.ext4 /dev/md0
mke2fs 1.42.9 (28-Dec-2013)
Filesystem label=
OS type: Linux
Block size=4096 (log=2)
Fragment size=4096 (log=2)
...
>>> echo "/dev/md0 /RAID ext4 defaults 0 0" >> /etc/fstab
>>> mkdir /RAID
>>> mount -a
```

然后我们把 `/dev/sdb` 移出阵列，这时备份盘会自动替换并开始数据同步：

```bash
>>> mdadm /dev/md0 -f /dev/sdb
mdadm: set /dev/sdb faulty in /dev/md0
>>> mdadm -D /dev/md0
/dev/md0:
Version : 1.2
Creation Time : Fri May 8 09:20:35 2017
...
Number Major Minor RaidDevice State
3 8 64 0 spare rebuilding /dev/sde
1 8 32 1 active sync /dev/sdc
4 8 48 2 active sync /dev/sdd
0 8 16 - faulty /dev/sdb
```

# 逻辑卷管理器 LVM

当我们把硬盘分好区或部署成磁盘阵列时，再想修改硬盘分区大小就很难了，这里就可以用硬盘设备资源管理技术—— LVM（逻辑卷管理器），它可以允许用户对硬盘资源进行动态调整。

![lvm](/assets/images/posts/2019-07-06-chap78/lvm.png)

LVM 是在硬盘分区和文件系统之间加了一个逻辑层，可以把多个硬盘进行卷组合并。相当于把面粉（物理卷[PV，Physical Volume]）揉成一个大面团（卷组[VG，Volume Group]），然后再把这个大面团分割成一个个小馒头（逻辑卷[LV，Logical Volume]），而且每个小馒头的重量必须是每勺面粉（基本单元[PE，Physical Extent]）的倍数。

## 部署逻辑卷

在生产环境中无法精确地评估每个硬盘分区在日后的使用情况，因此会导致原先分配的硬盘分区不够用。比如，伴随着业务量的增加，用于存放交易记录的数据库目录的体积也随之增加；因为分析并记录用户的行为从而导致日志目录的体积不断变大，这些都会导致原有的硬盘分区在使用上捉襟见肘。而且，还存在对较大的硬盘分区进行精简缩容的情况。

我们可以通过部署 LVM 来解决上述问题。部署 LVM 时，需要逐个配置物理卷、卷组和逻辑卷。

### 常用 LVM 部署命令

|------|------------|-----------|------------|
| 命令 | 物理卷管理 | 卷组管理 | 逻辑卷管理 |
|------|------------|-----------|------------|
| 扫描 | pvscan | vgscan | lvscan |
| 建立 | pvcreate | vgcreate | lvcreate |
| 显示 | pvdisplay | vgdisplay | lvdisplay |
| 删除 | pvremove | vgremove | lvremove |
| 扩展 | | vgextend | lvextend |
| 缩小 | | vgreduce | lvreduce |
|------|------------|-----------|------------|

### 部署流程

1. 在硬盘中创建物理卷

   ```bash
   >>> pvcreate /dev/sdb /dev/sdc
   cal volume "/dev/sdb" successfully created
   Physical volume "/dev/sdc" successfully created
   ```

2. 把两块硬盘设备加入卷组 storage 中

   ```bash
   >>> vgcreate storage /dev/sdb /dev/sdc
   Volume group "storage" successfully created

   >>>vgdisplay
   --- Volume group ---
   VG Name storage
   System ID
   Format lvm2
   Metadata Areas 2
   Metadata Sequence No 1
   VG Access read/write
   VG Status resizable
   MAX LV 0
   Cur LV 0
   Open LV 0
   Max PV 0
   Cur PV 2
   Act PV 2
   VG Size 39.99 GiB
   PE Size 4.00 MiB
   Total PE 10238
   Alloc PE / Size 0 / 0  Free PE / Size 10238 / 39.99 GiB
   VG UUID KUeAMF-qMLh-XjQy-ArUo-LCQI-YF0o-pScxm1
   ```

3. 划出 150MB 的逻辑卷设备

   ```bash
   >>> lvcreate -n logic_volumn -l 38 storage
   Logical volume "logic_volumn" created
   >>> lvdisplay
   gical volume ---
   LV Path /dev/storage/logic_volumn
   LV Name logic_volumn
   VG Name storage
   ...
   LV Size 148.00 MiB
   ...
   ```

   > 注意：
   >
   > - 逻辑卷最小单位：4MB
   > - `-l`：以 4MB 为计数单位
   > - `-L`：生成指定大小的逻辑卷

4. 格式化逻辑卷设备

   ```bash
   >>> mkfs.ext4 /dev/storage/logic_volumn
   mke2fs 1.42.9 (28-Dec-2013)
   Filesystem label=
   OS type: Linux
   ...
   >>> mkdir /volumn
   >>> mount /dev/storage/logic_volumn /volume
   ```

5. 查看状态

   ```bash
   >>> df -h
   Filesystem Size Used Avail Use% Mounted on
   /dev/mapper/rhel-root 18G 3.0G 15G 17% /
   ...
   /dev/mapper/storage-logic_volumn 145M 7.6M 138M 6% /volume
   >>> ehco "/dev/storage/logic_volumn /volume ext4 defaults 0 0" >> /etc/fstab
   ```

### 扩容逻辑卷 lvextend

1. 卸载设备

   ```bash
   >>> umount /Volumes
   ```

2. 扩容至 290 MB

   ```bash
   >>> lvextend -L 290MB /dev/storage/logic_volumn
   Rounding size to boundary between physical extents: 292.00 MiB
   Extending logical volume vo to 292.00 MiB
   Logical volume vo successfully resized
   ```

3. 检查磁盘，并通知内核更新磁盘容量

   ```bash
   >>> e2fsck -f /dev/storage/logic_volumn
   e2fsck 1.42.9 (28-Dec-2013)
   Pass 1: Checking inodes, blocks, and sizes
   Pass 2: Checking directory structure
   Pass 3: Checking directory connectivity
   Pass 4: Checking reference counts
   Pass 5: Checking group summary information
   /dev/storage/vo: 11/38000 files (0.0% non-contiguous), 10453/151552 blocks

   >>> resize2fs /dev/storage/logic_volumn
   resize2fs 1.42.9 (28-Dec-2013)
   Resizing the filesystem on /dev/storage/vo to 299008 (1k) blocks.
   The filesystem on /dev/storage/vo is now 299008 blocks long.
   ```

4. 重新挂载硬盘

   ```bash
   >>> mount -a
   >>> df -h
   Filesystem Size Used Avail Use% Mounted on
   /dev/mapper/rhel-root 18G 3.0G 15G 17% /
   devtmpfs 985M 0 985M 0% /dev
   tmpfs 994M 80K 994M 1% /dev/shm
   tmpfs 994M 8.8M 986M 1% /run
   tmpfs 994M 0 994M 0% /sys/fs/cgroup
   /dev/sr0 3.5G 3.5G 0 100% /media/cdrom
   /dev/sda1 497M 119M 379M 24% /boot
   /dev/mapper/storage-logic_volumn 279M 2.1M 259M 1% /volume
   ```

### 缩小逻辑卷 lvreduce

在缩小之前，先备份数据，并检查磁盘

1. 卸载设备

   ```bash
   >>> umount /Volume
   ```

2. 检查磁盘

   ```bash
   >>> e2fsck -f /dev/storage/logic_volumn
   e2fsck 1.42.9 (28-Dec-2013)
   Pass 1: Checking inodes, blocks, and sizes
   Pass 2: Checking directory structure
   Pass 3: Checking directory connectivity
   Pass 4: Checking reference counts
   Pass 5: Checking group summary information
   /dev/storage/logic_volumn: 11/74000 files (0.0% non-contiguous), 15507/299008 blocks
   ```

3. 缩小逻辑卷到 120MB

   ```bash
   >>> resize2fs /dev/storage/logic_volumn 120M
   resize2fs 1.42.9 (28-Dec-2013)
   Resizing the filesystem on /dev/storage/vo to 122880 (1k) blocks.
   The filesystem on /dev/storage/vo is now 122880 blocks long.
   >>> lvreduce -L 120M /dev/storage/logic_volumn
   WARNING: Reducing active logical volume to 120.00 MiB
   THIS MAY DESTROY YOUR DATA (filesystem etc.)
   Do you really want to reduce vo? [y/n]: y
   Reducing logical volume vo to 120.00 MiB
   Logical volume vo successfully resized
   ```

### 逻辑卷快照

LVM 有快照功能，特点：

- 快照卷的容量必须与逻辑卷容量相同
- 快照卷仅一次有效，一旦执行还原后立即删除

1.  查看卷组信息

    ```bash
    >>> vgdisplay
    --- Volume group ---
    VG Name storage
    System ID
    Format lvm2
    ...
    # 分配PE 30个，120MB，空闲39G
    Alloc PE / Size 30 / 120.00 MiB Free PE / Size 10208 / 39.88 GiB
    ```

2.  生成快照卷

    ```bash
    # -s：生成快照卷
    >>> lvcreate -L 120M -s -n SNAP /dev/storage/logic_volumn
    Logical volume "SNAP" created
    >>> lvdisplay
    --- Logical volume ---
    LV Path /dev/storage/SNAP
    LV Name SNAP
    VG Name storage
    LV UUID BC7WKg-fHoK-Pc7J-yhSd-vD7d-lUnl-TihKlt
    LV Write Access read/write
    LV Creation host, time localhost.localdomain, 2017-02-01 07:42:31 -0500
    LV snapshot status active destination for logic_volumn
    LV Status available
    # open 0
    LV Size 120.00 MiB
    Current LE 30
    COW-table size 120.00 MiB
    COW-table LE 30
    Allocated to snapshot 0.01%
    Snapshot chunk size 4.00 KiB
    ...
    ```

3.  在逻辑卷中创建 100MB 文件，查看快照卷，它的存储空间也上升了

        ```bash
        >>> dd if=/dev/zero of=/logic_volumn/files count=1 bs=100M
        1+0 records in

    1+0 records out
    104857600 bytes (105 MB) copied, 3.35432 s, 31.3 MB/s >>> lvdisplay
    --- Logical volume ---
    LV Path /dev/storage/SNAP
    LV Name SNAP
    VG Name storage
    LV UUID BC7WKg-fHoK-Pc7J-yhSd-vD7d-lUnl-TihKlt
    LV Write Access read/write
    LV Creation host, time localhost.localdomain, 2017-02-01 07:42:31 -0500
    LV snapshot status active destination for vo
    LV Status available # open 0
    LV Size 120.00 MiB
    Current LE 30
    COW-table size 120.00 MiB
    COW-table LE 30

# 快照卷容量也上升了

    Allocated to snapshot 83.71%
    ```

3. 用快照卷还原逻辑卷 `logic_volumn`

   ```bash
   # 首先要卸载要还原的lv
   >>> umount /logic_volumn
   >>> lvconvert --merge /dev/storage/SNAP
    Merging of volume SNAP started.
   vo: Merged: 21.4%
   vo: Merged: 100.0%
   Merge of snapshot into logical volume vo has finished.
   Logical volume "SNAP" successfully removed
   ```

   完成还原后，快照卷被删除

### 删除逻辑卷

当生产环境需要重新布置 LVM 或不再需要使用 LVM 时，需要执行删除操作。为此要备份好数据，然后依次删除逻辑卷、卷组、物理卷。

1. 取消挂载

   ```bash
   >>> umount /logic_volumn
   >>> vim /etc/fstab
   ```

2. 删除逻辑卷设备

   ```bash
   lvremove /dev/storage/logic_volumn
   Do you really want to remove active logical volume vo? [y/n]: y
   Logical volume "vo" successfully removed
   ```

3. 删除卷组

   ```bash
   >>> vgremove storage
   Volume group "storage" successfully removed
   ```

4. 删除物理卷设备

   ```bash
   >>> pvremove /dev/sdb /dev/sdc
    Labels on physical volume "/dev/sdb" successfully wiped
   Labels on physical volume "/dev/sdc" successfully wiped
   ```
